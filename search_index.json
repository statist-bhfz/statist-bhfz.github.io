[
["index.html", "Деревянные алгоритмы в R и Python Вступление", " Деревянные алгоритмы в R и Python Артем Груздев, Андрей Огурцов 2020-07-31 Вступление "],
["ch1.html", "1 Введение в метод деревьев решений 1.1 Введение в методологию деревьев решений 1.2 Краткий обзор методов деревьев решений CHAID и CART 1.3 Преимущества и недостатки деревьев решений 1.4 Задачи, выполняемые с помощью деревьев решений", " 1 Введение в метод деревьев решений 1.1 Введение в методологию деревьев решений Как и регрессионный анализ, деревья решений являются методом изучения статистической взаимосвязи между одной зависимой переменной и несколькими независимыми (предикторными) переменными. При этом под зависимой переменной понимается переменная, «поведение» которой мы хотим предсказать, а под предикторами подразумеваются переменные, которые помогают нам это сделать1. Базовое отличие метода деревьев решений от регрессионного анализа заключается в том, что взаимосвязь между значением зависимой переменной и значениями независимых переменных представлена не в виде общего прогнозного уравнения, а в виде древовидной структуры, которую получают с помощью иерархической сегментации данных. Берется весь обучающий набор данных, называемый корневым узлом, и разбивается на два или более узлов (сегментов) так, чтобы наблюдения, попавшие в разные узлы, максимально отличались друг от друга по зависимой переменной (например, выделяем два узла с наибольшим и наименьшим процентом «плохих» заемщиков). В роли правил разбиения, максимизирующих эти различия, выступают значения независимых переменных (пол, возраст, доход и др.). Качество разбиения оценивается с помощью статистических критериев. Обычно оценка качества разбиения происходит в два этапа. На первом этапе по каждому предиктору осуществляется наилучшее правило разбиения, а на втором этапе из наилучших правил, найденных по каждому предиктору на первом этапе, выбирается самое лучшее. Правила отмечаются на ветвях – линиях, которые соединяют разбиваемый узел с узлами, полученными в результате разбиения. Для каждого узла вычисляются вероятности в виде процентных долей категорий зависимой переменной (если зависимая переменная является категориальной) или средние значения зависимой переменной (если зависимая переменная является количественной). В результате выносится решение – спрогнозированная категория зависимой переменной (если зависимая переменная является категориальной) или спрогнозированное среднее значение зависимой переменной (если зависимая переменная является количественной). Аналогичным образом, каждый узел, получившийся в результате разбиения корневого узла, разбивается дальше на узлы, т.е. узлы внутри узла, и т.д. Этот процесс продолжается до тех пор, пока есть возможность создания новых узлов. Данный процесс сегментации называется рекурсивным разделением. Получившаяся иерархическая структура, характеризующая взаимосвязь между значением зависимой переменной и значениями независимых переменных, называется деревом. Иногда для обозначения разбиваемого узла применяется термин родительский узел. Новые узлы, получившиеся в результате разбиения, называются дочерними узлами (или узлами-потомками). Когда впоследствии дочерний узел разбивается сам, он становится родительским узлом. Окончательные узлы, которые в дальнейшем не разбиваются, называются терминальными узлами (листьями) дерева. Лист представляет собой наилучшее окончательное решение, выдаваемое деревом. Здесь мы определяем группы клиентов, обладающие желаемыми характеристиками (например, тех, кто погасит кредит или откликнется на наше маркетинговое предложение). В случае, когда вы прогнозируете вероятность значения категориальной зависимой переменной по соответствующим значениям предикторов, дерево решений называют деревом классификации (рис. 1.1). Например, дерево классификации строится для вычисления вероятности отклика у заемщика (на основе спрогнозированной вероятности мы относим его к неоткликнувшемуся – к классу 0 или к откликнувшемуся – классу 1). Если дерево решений используется для того, чтобы спрогнозировать среднее значение количественной зависимой переменной по соответствующим значениям предикторов, его называют деревом регрессии (рис. 1.2). Например, дерево регрессии строится, чтобы вычислить средний размер вклада у клиента. Рис. 1.1. Дерево классификации (на примере пакета R rpart) Рис. 1.2. Дерево регрессии (на примере пакета R rpart) Если визуализировать работу алгоритма дерева решений, то мы увидим, что алгоритм последовательно разбивает данные на прямоугольники, параллельные осям координат. Проиллюстрируем это на примере бинарного дерева решений, то есть случая, когда узел-родитель может иметь только два узла-потомка. У нас есть набор данных, состоящий из 32 наблюдений, предикторами являются переменные Длительность звонков в минутах и Количество обращений в службу поддержки, зависимая переменная – Статус клиента: 18 клиентов относятся к классу Ушедший, а 14 – к классу Оставшийся. Рис. 1.3. Визуальное представление набора данных Первое разбиение набора данных происходит по предиктору Количество обращений в службу поддержки. Рис. 1.4. Разбиение набора данных по предиктору Количество обращений в службу поддержки Затем каждый из полученных узлов разбивается по предиктору Длительность звонков в минутах, и дерево останавливается в росте. Рис. 1.5. Процесс последовательного разбиения набора данных Таким образом, можно сделать вывод: если клиент обращался в службу поддержки 6 раз и более и при этом делал звонки длительностью 15 минут и более, его можно отнести к классу ушедших клиентов. Вполне возможно, что постоянно возникающие проблемы в оказании услуг (об этом свидетельствует факт частого обращения в службу поддержки) при высокой интенсивности использования сотового телефона стали причиной оттока. Ниже приводится рис. 1.6, на котором показаны все границы принятия решений, предложенные деревом для нашего набора данных. Рис. 1.6. Границы принятия решений, предложенные деревом Обратите внимание, что деревья решений применяют различные критерии для отбора предикторов и разбиения узлов в зависимости от шкалы зависимой и независимых переменных2. Поэтому важно задать правильные шкалы для всех переменных. В R переменные могут иметь три типа шкалы: количественную, порядковую и номинальную. Подготовку данных в Python мы будем выполнять с помощью библиотеки pandas, в которой переменные могут иметь два типа шкалы: количественную и категориальную. Количественная шкала допускает возможность сравнения значений: мы можем узнать, на сколько и во сколько раз одно значение больше/меньше другого, при этом второй тип сравнения не всегда возможен. Среди количественных шкал выделяют шкалу интервалов, шкалу отношений и абсолютную шкалу. Шкала интервалов состоит из одинаковых интервалов и имеет условную нулевую точку (точку отсчета). Она позволяет сказать, насколько одно значение больше другого, но не позволяет сказать, во сколько раз оно больше. Например, повысив температуру с 1°C до 20°C, мы можем сказать, что температура 20°C на 19 градусов Цельсия больше 1°C, но не можем сказать, что температура 20°C в 20 раз больше, чем 1°C. Из школьного курса физики вспомним, что температура среды (например, воздуха) определяется энергией молекул, составляющих эту среду. Для идеального газа внутренняя энергия равна сумме кинетических энергий его молекул, которая, в свою очередь, пропорциональна абсолютной температуре в кельвинах. Очевидно, что, например, при «двадцатикратном» нагреве с 1°C до 20°C абсолютная температура изменится всего в (273 + 20) / (273 + 1) = 1,069 раза. Ноль по шкале Цельсия условен и соответствует 273К. Шкала отношений отличается от шкалы интервалов тем, что имеет естественную нулевую точку. Она позволяет сказать, насколько одно значение больше другого и во сколько раз оно больше. Примером шкалы отношений может служить переменная Возраст: мы знаем, что расстояние между 25 и 30 в два раза меньше, чем расстояние между 30 и 40, 30-летний на 5 лет старше 25-летнего. Шкалы большинства физических величин (длина, масса, сила, давление, скорость и др.) являются шкалами отношений. При этом единица измерения в этих шкалах может быть произвольной. Например, возраст можно измерять в годах, месяцах, неделях. Длину мы можем измерять в километрах, милях, лье. Абсолютная шкала помимо естественной нулевой точки имеет еще и естественную общепринятую единицу измерения. Пример абсолютной шкалы – абсолютная шкала температуры или шкала Кельвина. Нуль этой шкалы отвечает полному прекращению движения молекул, т.е. самой низкой температуре, а единицей измерения является кельвин, который равен 1/273,16 части термодинамической температуры тройной точки воды. Как и шкала отношений, абсолютная шкала также позволяет сказать, насколько одно значение больше другого и во сколько раз оно больше. При работе с количественными шкалами мы можем упорядочить значения по нарастанию или убыванию интенсивности определенного признака (например, по увеличению возраста): после 25 следует 30 и 30-летний старше 25-летнего. Наконец, мы можем сказать, сколько в выборке человек с тем или иным значением возраста. В R количественным переменным соответствуют векторы типа double, когда значения представлены в виде чисел с плавающей точкой, например, доход клиента составляет 36500.60 рублей (такие векторы еще называют числовыми), и векторы типа integer, когда значения представлены в виде целых чисел, например, доход клиента составляет 36000 рублей (такие векторы еще называют целочисленными). Числовые и целочисленные векторы относятся к классу numeric. В питоновской библиотеке pandas количественным переменным будут соответствовать переменные типа float (аналог векторов типа double в R) и переменные типа int (аналог векторов типа integer в R). Для порядковой шкалы задан лишь порядок или ранжирование значений. Допускаются сравнения между значениями, но нельзя сказать, на сколько или во сколько раз одно значение больше другого. Пример предиктора с порядковой шкалой – переменная Доход, принимающая значения низкий, средний, высокий. Здесь уже нельзя утверждать, что расстояние между значениями низкий и средний больше или меньше в определенное количество раз расстояния между значениями средний и высокий. Мы не можем утверждать, что человек со средним доходом на n-ное количество единиц богаче, чем человек с низким доходом. Однако можно упорядочить значения по нарастанию или убыванию интенсивности определенного признака: сначала следует значение низкий, затем значение средний и потом значение высокий. Респонденты, относящиеся к значению средний, обладают меньшим доходом по сравнению с респондентами, относящимися к значению высокий, то есть демонстрируют меньшую интенсивность признака. Также мы можем сказать, сколько в выборке человек с тем или иным уровнем дохода. В R порядковым переменным соответствуют векторы, относящиеся одновременно к типам ordered и factor (наборы упорядоченных числовых кодов, предназначенных для хранения строковых значений). Их называют упорядоченными факторами. В питоновской библиотеке pandas для порядковой переменной не предусмотрено отдельного типа, поэтому такие переменные представляют как переменные типа int или переменные типа object. Номинальная шкала содержит только информацию о количестве объектов в значениях. Пример предиктора с номинальной шкалой – переменная Регион, который имеет уровни Алтайский край, Новосибирская область, Красноярский край, Кемеровская область. Мы ничего не можем сказать о расстояниях между значениями или о порядке значений. Мы можем лишь судить о количестве респондентов, проживающих в каждом регионе. В R номинальным переменным соответствуют векторы типа factor (наборы неупорядоченных числовых кодов, предназначенных для хранения строковых значений). В библиотеке pandas номинальным переменным будут соответствовать переменные типа object; также для этого можно использовать тип str. Рис. 1.7. Три типа шкал 1.2 Краткий обзор методов деревьев решений CHAID и CART В настоящее время наиболее распространенными методами деревьев решений являются CHAID и CART. CHAID (Chi-square Automatic Interaction Detector – автоматический обнаружитель взаимодействий) был разработан Гордоном Каасом в 1980 году и представляет собой метод на основе дерева решений, который исследует взаимосвязь между предикторами и зависимой переменной с помощью статистических тестов. Каждый раз для разбиения узла выбирается предиктор, сильнее всего взаимодействующий с зависимой переменной. Перед этим категории каждого предиктора объединяются, если они не имеют между собой статистически значимых отличий по отношению к зависимой переменной. Категории, которые дают значимые отличия по зависимой переменной, рассматриваются как отдельные. Зависимая переменная может быть измерена в категориальной шкале. Предикторы могут быть только категориальными переменными (количественные переменные должны быть предварительно преобразованы в категориальные с помощью биннинга). CHAID позволяет осуществлять многомерные расщепления узлов. Каждый узел при разбиении может иметь более 2 потомков, поэтому CHAID имеет тенденцию выращивать более раскидистые деревья, чем бинарные методы. Вместе с тем из-за жестких статистических критериев расщепления нередко дерево CHAID получается нереалистично коротким и тривиальным («грубое» дерево), поэтому требуется тонкая настройка уровней значимости для объединения категорий и разбиения узлов. По сравнению с другими методами CHAID характеризуется умеренным временем вычислений. Помимо прочего, метод CHAID обладает собственным способом обработки пропущенных значений. Пропуски рассматриваются как отдельная фактическая категория. В ряде случаев это имеет смысл. Например, отказ отвечать на вопрос о доходе или занятости может оказаться предсказательной категорией для зависимой переменной. CART (Classification and Regression Tree – деревья классификации и регрессии) был разработан в 1974-1984 годах профессорами статистики Лео Брейманом (Калифорнийский университет в Беркли), Джеромом Фридманом (Стэнфордский университет), Ричардом Олшеном (Калифорнийский университет в Беркли) и Чарльзом Стоуном (Стэнфордский университет). Для построения дерева метод CART использует меры неоднородности. Эти меры основаны на принципе уменьшения неоднородности в узле. Каждый раз расщепление узла происходит так, чтобы узлы-потомки стали более однородными, чем его узел-родитель. В абсолютно однородном узле все наблюдения имеют одно и то же значение целевой переменной (все объекты принадлежат к одной и той же категории целевой переменной). Такой узел называют «чистым», и в нем мы получаем максимально однозначный прогноз. Зависимая переменная и предикторы может быть категориальными или количественными. CART позволяет выполнять только одномерные расщепления узлов. Каждый узел при разбиении может иметь только 2 потомков. Поэтому CART имеет тенденцию выращивать высокие деревья с большим количеством уровней. Часто деревья CART получаются слишком детализированные, имеют много узлов и ветвей, сложны для интерпретации, при этом усложнение дерева не приводит к повышению прогностической способности дерева. Для упрощения структуры дерева и повышения качества модели в методе CART предусмотрена возможность отсечения ветвей (прунинг). Прунинг позволяет получить дерево «подходящего размера», избежать построения ветвистых, усложненных деревьев и при этом достичь лучшего качества модели. Для обработки наблюдений, у которых пропущено значение в предикторе, используются суррогаты – другие предикторы, имеющие сильную корреляцию с исходной независимой переменной. Таким образом, разбиение, задаваемое суррогатом, будет наиболее близко к разбиению, задаваемому исходным предиктором, по которому имеются пропуски. Метод CART требует большее время вычислений по сравнению с другими методами. Ниже на рис. 1.8 приводится таблица сходств и различий между методами CHAID и CART. Рис. 1.8. Отличие методов CHAID и CART 1.3 Преимущества и недостатки деревьев решений Метод деревьев решений обладает рядом преимуществ. Главное из них – это наглядность представления результатов (в виде иерархической структуры дерева). Деревья решений позволяют работать с большим числом независимых переменных. На вход можно подавать все существующие переменные, алгоритм сам выберет наиболее значимые среди них, и только они будут использованы для построения дерева (автоматический отбор предикторов). Однако при этом для некоторых методов (например, для CHAID) может потребоваться категоризация количественных предикторов, при которой теряется часть содержащейся в ней информации. Деревья решений бывают более эффективны по сравнению с линейными моделями в тех случаях, когда взаимосвязи между предикторами и зависимой переменной является нелинейными, наблюдается большое количество коррелирующих между собой переменных, присутствуют взаимодействия высоких порядков. Это обусловлено тем, что деревья пытаются описать связь между переменными путем многократных разбиений по предикторам. CHAID делает это за счет расщепления сразу на несколько категорий, CART пытается уловить эту связь посредством серии бинарных делений, и это может быть менее эффективно по сравнению с подбором параметров в линейных моделях. Деревья решений устойчивы к выбросам, поскольку разбиения основаны на количестве наблюдений внутри диапазонов значений, выбранных для расщепления, а не на абсолютных значениях. Например, если у нас есть наблюдение со значением 99999, дерево может создать два узла с правилами «&lt;5» и «&gt;5» и отнести наблюдение со значением 99999 в правый узел. Деревья решений относительно нечувствительны к наличию мультиколлинеарности (сильной корреляции между предикторами), в то время как линейным моделям в подобной ситуации понадобится правильно подобранная регуляризация. Перед построением модели не обязательно импутировать пропущенные значения, поскольку, как уже говорилось выше, деревья используют собственные процедуры обработки пропусков. Требования, выдвигаемые методом деревьев решений к распределению переменных, не являются строгими. Теперь о недостатках3. Для деревьев решений нельзя получить простое общее прогнозное уравнение, выражающего модель (в отличие от линейных моделей). Еще одним недостатком можно считать принципиальную неспособность деревьев выполнять экстраполяцию при решении задачи регрессии: предсказание для любой комбинации предикторов всегда будет средним значением в одном из листьев, т.е. за пределы диапазона значений целевой переменной в обучающей выборки выйти невозможно. Линейная модель, наоборот, может дать осмысленный прогноз (хотя качество этого прогноза не гарантируется – модели машинного обучения ведут предсказуемо себя в той области значений предикторов и целевой переменной, которые модель «видела» на этапе обучения). Деревьям решений свойственно переобучение. Речь идет о ситуации, когда дерево в силу гибкости используемого алгоритма получается слишком детализированным, имеют много узлов и ветвей, сложно для интерпретации, что требует специальной процедуры отсечения. Небольшие изменения в наборе данных могут приводить к построению совершенно другого дерева, потому что в силу иерархичности дерева изменения в верхних узлах ведут к изменениям во всех узлах, расположенных ниже. Отметим, что в большей степени проблемы переобучения и нестабильности относится к методу CART (в CHAID эти проблемы во многом снимаются за счет использования строгих статистических критериев). Предпринимая попытки улучшить прогностическую способность и стабильность метода CART, один из его разработчиков Лео Брейман пришел к идее случайного леса, когда из обучающего набора извлекаются случайные выборки (того же объема, что и исходный обучающий набор) с возвращением, по каждой строится дерево с использованием случайно отобранных предикторов и затем результаты, полученные по каждому дереву, усредняются. Однако при таком подходе теряется главное преимущество деревьев решений – простота интерпретации. Наконец, для методов одиночных деревьев характерна проблема множественных сравнений. Перед расщеплением узла дерево сравнивает различные варианты разбиения, число этих вариантов зависит от числа уровней предикторов, поэтому, как правило, происходит смещение выбора в пользу переменных, у которых большее количество уровней. Рис. 1.9. Преимущества и недостатки деревьев решений 1.4 Задачи, выполняемые с помощью деревьев решений Деревья применяются для задач регрессии и классификации в различных областях – от банковской сферы до медицины. Также они могут быть использованы, например, сегментации клиентской базы: определив, какие демографические группы имеют максимальный показатель отклика, мы можем использовать эту информацию, чтобы максимизировать отклик при будущей прямой рассылке. Здесь необходимо отметить, что в настоящее время дерево решений CART все реже используется для построения самостоятельной прогнозной модели в силу вышеописанных недостатков, а построение модели на основе дерева CHAID должно сопровождаться тщательно подготовленными процедурами проверки. Часто дерево решений CHAID используют в ансамбле с логистической регрессией. Например, ансамбли дерева решений CHAID и логистической регрессии используются в моделях оттока в телекоме, что позволяет поднять качество модели, при этом, в отличие от случайного леса и градиентного бустинга, такой ансамбль легко интерпретируем. В банковском скоринге деревья решений используются как вспомогательный инструмент при разработке модели логистической регрессии. Приведем конкретные примеры такого применения дерева. В кредитном скоринге использование нескольких скоринговых карт для одного портфеля обеспечивает лучшее дифференцирование риска, чем использование одной скоринговой карты. Это характерно, когда нам приходится работать с разнородной аудиторией, состоящей из различных групп, и одна и та же скоринговая карта не может работать достаточно эффективно для всех. Например, в скоринге кредитных карточек выделяют сегменты «активные клиенты» и «неактивные клиенты», «клиенты в просрочке» и «клиенты, не имеющие просрочек». Переменные в таких сегментах будут сильно различаться. Например, для активных кредитных карт утилизация будет сильной переменной, а для неактивных – слабой. И, наоборот, может оказаться, что время неактивности для активных клиентов равно 0, а для неактивных клиентов время неактивности окажется сильной переменной. Для этих целей выполняют сегментацию клиентов. Первый способ сегментации – деление на группы на основе опыта и отраслевых знаний с последующей аналитической проверкой. Второй способ – это сегментация с помощью статистических методов типа кластерного анализа или деревьев решений. При этом по сравнению с кластерным анализом деревья решений обладают преимуществом: они формулируют четкие правила выделения сегментов. В дальнейшем для каждого из сегментов можно построить собственную модель логистической регрессии, разработать скоринговую карту и сформулировать кредитные правила. В Citibank USA является стандартной практикой делать дерево с двумя-тремя уровнями и в каждом узле подгонять свою модель логистической регрессии. В основе скорингового балла FICO также лежит сегментация на основе деревьев решений. Об эффективности использования сегментации в кредитном скоринге пишет в своей книге «Скоринговые карты для оценки кредитных рисков» известный эксперт по управления рисками Наим Сиддики4, а также один из разработчиков алгоритмов скоринга компании FICO Брюс Ходли5. С помощью деревьев решений из большого числа предикторов можно выбрать переменные, полезные для построения модели логистической регрессии). Например, из 100 переменных дерево включило в модель 25 переменных, таким образом, у нас появляется информация о том, какие переменные наверняка можно включить в модель логистической регрессии. Методы CART и случайный лес позволяют вычислить важность переменных, использованных в модели, благодаря чему мы можем ранжировать переменные по степени полезности. Деревья решений можно использовать для биннинга – перегруппировки категориального предиктора или дискретизации количественного предиктора с целью лучшего описания взаимосвязи с зависимой переменной. Например, при построении модели логистической регрессии часто обнаруживается, что взаимосвязи между количественным предиктором и интересующим событием являются нелинейными. Уравнение логистической регрессии, несмотря на нелинейное преобразование своего выходного значения (логит-преобразование), все равно моделирует линейные зависимости между предикторами и зависимой переменной. Возьмем пример нелинейной зависимости между стажем работы в банке и внутренним мошенничеством. Допустим, рассчитанный регрессионный коэффициент в уравнении логистической регрессии получился отрицательным. Это значит, что вероятность совершения внутреннего мошенничества с увеличением стажа работы уменьшается. Однако, после выполнения разбивки переменной с помощью дерева CHAID на категории до 12 месяцев, от 12 до 36 месяцев, от 36 до 60 месяцев и больше 60 месяцев, стало видно, что зависимость между стажем и внутренним мошенничеством нелинейная. Первая (до 12 месяцев) и последняя (больше 60 месяцев) категории склонны к внутреннему мошенничеству, а промежуточные сегменты, наоборот, не склонны к внутреннему мошенничеству. После правильной разбивки переменной, проведенной с помощью дерева, связь между предиктором и зависимой переменной становится больше похожа на реальную. Строя модель логистической регрессии, нередко приходится работать с предикторами, у которых большое количество категорий. Как правило, речь идет о географических переменных (регион, область регистрации, область фактического пребывания заемщика, область торговой точки, где клиент брал кредит) и переменных, фиксирующих профессию или сферу занятости заемщика. При этом некоторые категории могут быть редкими и либо станут источником шума, либо вообще вызовут проблемы моделирования (когда наблюдения, относящиеся к редкой категории, не попадут в обучающую выборку, но встретятся в контрольной). Исключение таких переменных из анализа также нерационально, поскольку они могут дать ценную информацию. Поэтому можно выполнить биннинг с целью укрупнения категорий, а можно построить по такой переменной дерево решений. В результате дерево укрупнит категории переменных и скомбинирует переменные так, чтобы полученные комбинации характеристик максимизировали различия по зависимой переменной. Такую переменную, где категориями являются терминальные узлы дерева, можно включить в модель логистической регрессии. Поэтому зависимую переменную еще называют объясняемой переменной, а предикторы – объясняющими переменными. Кроме того, в качестве синонима «зависимая переменная» могут использоваться термины «целевая переменная», «результирующая переменная», «отклик», «выходная переменная». Предикторы также называют факторами, входными переменными.↩︎ Подробнее о шкалах см., например, по ссылке http://www.aiportal.ru/articles/autoclassification/type-scales.html.↩︎ Более подробное обсуждение этой темы можно найти по ссылке https://stats.stackexchange.com/questions/1292/what-is-the-weak-side-of-decision-trees↩︎ Наим Сиддики. Скоринговые карты для оценки кредитных рисков. М.: Манн, Иванов и Фабер, 2014.↩︎ Breiman, L. 2001. Arcing classifiers (with discussion). The Annals of Statistics, vol.26, no. 3, 801–849. 2001b. Statistical modeling: The two cultures (with rejoinders). Statistical Science, vol. 16, no. 3, 199–231.↩︎ "],
["ch2.html", "2 Построение деревьев решений CHAID с помощью пакета R CHAID 2.1 Знакомство с методом CHAID 2.2 Предварительная подготовка данных перед построением модели дерева CHAID", " 2 Построение деревьев решений CHAID с помощью пакета R CHAID 2.1 Знакомство с методом CHAID 2.1.1 Описание алгоритма Перед началом работы алгоритма CHAID необходимо преобразовать все имеющиеся количественные предикторы в порядковые переменные. Обычно их разбивают на 10 категорий одинакового объема. Алгоритм приступает к построению дерева, итеративно применяя к каждому узлу, начиная с корневого, процедуры объединения категорий, расщепления узла и проверки правил остановки. Этап 1. Объединение категорий 1. Для каждого предиктора с числом категорий больше двух6 алгоритм ищет пару категорий с наименее значимыми различиями по зависимой переменной, т.е. пару категорий, для которых после применения соответствующего статистического критерия получено наибольшее p-значение. Выбор статистического критерия определяется типом шкалы зависимой переменной. Для номинальной зависимой переменной используется критерий хи-квадрат Пирсона. Алгоритм строит двухвходовую таблицу сопряженности с категориями предиктора в качестве строк и категориями зависимой переменной в качестве столбцов. Он проверяет нулевую гипотезу о том, что категории предиктора не отличаются друг от друга с точки зрения распределения категорий зависимой переменной. Для количественной зависимой переменной используется F-критерий. Алгоритм осуществляет однофакторный дисперсионный анализ и проверяет нулевую гипотезу о том, что средние значения зависимой переменной для различных категорий предиктора не различаются между собой. ПРИМЕЧАНИЕ Пакет R CHAID позволяет работать только с категориальной зависимой переменной. Возможность работы с количественной зависимой переменной реализована в процедуре Деревья классификации/CHAID проприетарного программного пакета IBM SPSS Statistics и питоновском пакете CHAID. 2. Найдя наибольшее p-значение для пары категорий, алгоритм сравнивает его с заданным уровнем значимости для объединения категорий. Если p-значение: меньше или равно заданному уровню значимости для объединения категорий – алгоритм переходит к вычислению скорректированных p-значений для полученного набора категорий (шаг 3); больше уровня значимости для объединения категорий – эта пара объединяется в отдельную составную категорию, в результате формируется новый набор категорий предиктора и процесс начинается заново с поиска пары категорий с наибольшим p-значением. ПРИМЕЧАНИЕ В пакете R CHAID уровень значимости для объединения категорий можно задать c помощью параметра alpha2 вспомогательной функции chaid_control(). (Опциональный шаг) Если новая составная категория состоит из трех и более исходных категорий, алгоритм находит внутри этой составной категории наилучшее бинарное расщепление, которое дает наименьшее p-значение. Алгоритм выполняет бинарное расщепление, если его p-значение не превышает уровня значимости для разбиения объединенных категорий. ПРИМЕЧАНИЕ В пакете R CHAID уровень значимости для разбиения уже объединенных категорий можно настроить c помощью параметра alpha3 вспомогательной функции chaid_control(). 3. Получив сформированный набор категорий предиктора, алгоритм для категориальной зависимой переменной вновь строит двухвходовую таблицу сопряженности с категориями предиктора в качестве строк и категориями зависимой переменной в качестве столбцов, а для количественной зависимой переменной вновь выполняет однофакторный дисперсионный анализ. В результате алгоритм вычисляет скорректированное p-значение критерия хи-квадрат или F-критерия как исходное p-значение, умноженное на поправку Бонферонни. Поправка Бонферрони представляет собой корректировку уровня значимости в зависимости от числа возможных способов, с помощью которых исходные категории предиктора могут быть объединены в итоговые категории. Этап 2. Расщепление узла После вычисления скорректированных p-значений для итоговых наборов категорий по всем предикторам алгоритм переходит к этапу расщепления узла. 1. На этапе расщепления алгоритм выбирает, какой предиктор обеспечит наилучшее разбиение узла. Для этого предиктор должен иметь наименьшее скорректированное p-значение (т.е. должен являться наиболее статистически значимым). 2. Найдя предиктор с наименьшим скорректированным p-значением, алгоритм сравнивает его с заданным уровнем значимости для расщепления. Если p-значение: меньше или равно заданному уровню значимости для расщепления – алгоритм разбивает узел с использованием данного предиктора; больше заданного уровня значимости для расщепления, то алгоритм не расщепляет узел и узел рассматривается как терминальный. ПРИМЕЧАНИЕ В пакете R CHAID уровень значимости для расщепления узла можно настроить c помощью параметра alpha4 вспомогательной функции chaid_control(). Этап 3. Остановка Алгоритм проверяет, нужно ли прекратить построение дерева, в соответствии со следующими правилами остановки: Если узел стал однородным, то есть все наблюдения в узле имеют одинаковые значения зависимой переменной, узел не разбивается. Если текущая глубина дерева достигает заданной пользователем максимальной глубины дерева, процесс построения дерева останавливается. Если количество наблюдений в родительском узле меньше заданного пользователем минимума наблюдений в родительском узле, узел не разбивается. Если минимальное абсолютное количество наблюдений в терминальном узле меньше заданного пользователем минимума наблюдений в терминальном узле, узел не разбивается. Если минимальная относительная частота наблюдений в терминальном узле меньше заданной пользователем минимальной относительной частоты наблюдений в терминальном узле, узел не разбивается. ПРИМЕЧАНИЕ В пакете R CHAID с помощью ряда параметров вспомогательной функции chaid_control() можно изменить некоторые вышеперечисленные правила остановки: minsplit задает минимальное количество наблюдений в родительском узле перед расщеплением, по умолчанию 20; minbucket задает минимальное абсолютное количество наблюдений в терминальном узле, по умолчанию 7; minprob задает минимальную относительную частоту наблюдений в терминальном узле, по умолчанию 0.01; maxheight задает максимальную высоту или глубину дерева (количество уровней дерева, лежащих ниже корневого узла), по умолчанию равен -1, т.е. ограничение отсутствует. 2.1.2 Немного о критерии хи-квадрат Предположим, на этапе объединения алгоритм проверяет, различаются ли значимо две категории предиктора Семейное положение “Холост” и “Женат” по зависимой переменной Отклик. Нулевая гипотеза звучит так: категории предиктора не отличаются друг от друга с точки зрения распределения категорий зависимой переменной. Альтернативная гипотеза заключается в том, что категории предиктора все же отличаются друг от друга по зависимой переменной. Строится двухвходовая таблица сопряженности, где строки являются категориями предиктора Семейное положение, а столбцы – категориями зависимой переменной Отклик. Для каждой ячейки таблицы фиксируем наблюдаемую частоту. Затем для каждой ячейки фиксируем ожидаемую частоту согласно нулевой гипотезе. В итоге для каждой ячейки вычисляем квадрат разности между наблюдаемой и ожидаемой частотой, поделенный на ожидаемую частоту. Складываем результаты, вычисленные по каждой ячейке, и получаем значение хи-квадрат (\\(\\chi^{2}\\)). Процесс вычисления хи-квадрат проиллюстрирован на рис. 2.1. Рис. 2.1. Процесс вычисления значения хи-квадрат \\[\\chi^{2}=\\sum\\frac{(O-E)^2}{E},\\] где \\(O\\) – наблюдаемые частоты; \\(E\\) – ожидаемые частоты. \\[\\chi^{2}=\\frac{(20-16,5)^2}{16,5} + \\frac{(13-16,5)^2}{16,5} + \\frac{(30-33,5)^2}{33,5} + \\frac{(37-33,5)^2}{33,5}=2,216\\] Статистика хи-квадрат подчиняется распределению хи-квадрат со степенями свободы \\(df=(R–1)(C–1)\\), где \\(R\\) и \\(C\\) – количество строк и столбцов в таблице сопряженности. В нашем случае количество степеней свободы будет равно \\(df=(2–1)(2–1)=1\\). Чтобы выяснить, достаточно ли велико полученное значение хи-квадрат для отклонения нулевой гипотезы, вычисляем соответствующую ему p-значение. p-значение – это вероятность ошибки, заключающейся в отклонении нулевой гипотезы, когда она верна. Это вероятность того, что случайная величина, имеющая распределение хи-квадрат при условии верности нулевой гипотезы, примет значение, не меньшее, чем вычисленное значение хи-квадрат. Решение об отклонении нулевой гипотезы принимается в результате сравнения p-значения с определенным пороговым уровнем, который называют уровнем значимости (\\(\\alpha\\)). Обычно p-значение сравнивают с общепринятым стандартным уровнем значимости\\(\\alpha=0,05\\). Если найденное p-значение меньше уровня значимости, нулевую гипотезу отклоняют, в противном случае у нас нет оснований отвергнуть нулевую гипотезу. В нашем случае значение хи-квадрат 2,216 с одной степенью свободы соответствует р-значению 0,1366. Таким образом, вероятность того, что статистика хи-квадрат примет вычисленное значение 2,216 и выше, когда категории предиктора Семейное положение не отличаются друг от друга с точки зрения распределения категорий зависимой переменной Отклик, составляет 0,1366. Это превышает уровень значимости 0,05. У нас нет оснований отвергнуть нулевую гипотезу. Можно сделать вывод, что категории переменной Семейное положение действительно не отличаются друг от друга с точки зрения распределения неоткликнувшихся и откликнувшихся клиентов. Данные категории предиктора можно объединить. Обратите внимание, фраза «нет оснований отклонить нулевую гипотезу» не тождественна фразе «принять нулевую гипотезу», которая является неверной. Нулевая гипотеза обычно имеет очень конкретную формулировку. Например, она может звучит так: нет разницы между средним значением выборки №1 и выборки №2. Если мы не можем отклонить нулевую гипотезу, значит ли это, что данные значения равны? Вовсе не обязательно. То, что нам не удалось найти статистически значимой разницы, совершенно не означает, что мы доказали равенство двух величин. Кроме того, результаты применения статистических критериев зависят от величины различий и от размера выборки, и одинаковые различия на выборках разного размера могут оказаться в одном случае незначимыми (например, если есть две выборки по 20 наблюдений), а в другом (когда наблюдений будет по 1000) – значимыми на том же уровне значимости. Таким образом, мощность статистического критерия (способность выявлять различия там, где они есть) зависит от объема выборки. Важным практическим моментом в построении деревьев по методу CHAID является то, что по мере роста дерева в узлах остается все меньше и меньше наблюдений, и на определенном этапе мы теряем возможность провести очередное разделение просто в силу малого числа наблюдений в узле. Даже если такое деление могло бы улучшить качество модели на обучающей выборке, оно не будет произведено, если мы не сможем отклонить нулевую гипотезу на заданном уровне значимости; использование поправки Бонферрони только усугубляет эту ситуацию (см. ниже). Но это нельзя однозначно рассматривать как недостаток метода, поскольку описанная особенность является своего рода встроенной регуляризацией, которая может повысить обобщающую способность модели. 2.1.3 Немного об F-критерии Предположим, на этапе объединения категорий алгоритм проверяет, различаются ли значимо категории предиктора Семейное положение “Холост” и “Женат” по количественной зависимой переменной Доход. Нулевая гипотеза будет звучат так: средние значения зависимой переменной в категориях предиктора или группах одинаковы. Чтобы проверить ее, нужно ответить на два вопроса: насколько сильно значения отклоняются от среднего значения зависимой переменной в группах и насколько сильно средние значения зависимой переменной в группах отличаются от среднего значения зависимой переменной перед разбиением на группы. Соответственно выполняется однофакторный дисперсионный анализ, в ходе которого подсчитывают внутригрупповую сумму квадратов отклонений и межгрупповую сумму квадратов отклонений и вычисляют F-критерий (критерий Фишера). Сумма квадратов между группами (дочерними узлами) определяется по формуле: Предположим, на этапе объединения категорий алгоритм проверяет, различаются ли значимо категории предиктора Семейное положение Холост и Женат по количественной зависимой переменной Доход. Нулевая гипотеза будет звучат так: средние значения зависимой переменной в категориях предиктора или группах одинаковы. Чтобы проверить ее, нужно ответить на два вопроса: насколько сильно значения отклоняются от среднего значения зависимой переменной в группах и насколько сильно средние значения зависимой переменной в группах отличаются от среднего значения зависимой переменной перед разбиением на группы. Соответственно выполняется однофакторный дисперсионный анализ, в ходе которого подсчитывают внутригрупповую сумму квадратов отклонений и межгрупповую сумму квадратов отклонений и вычисляют F-критерий (критерий Фишера). Сумма квадратов между группами (дочерними узлами) определяется по формуле: \\[SS_{межгрупп}=\\sum^{B}_{i=1}n_i(\\bar{y}_{i.}-\\bar{y}_{..})^2,\\] где \\(\\bar{y}_{i.}\\) – среднее значение зависимой переменной в \\(i\\)-том дочернем узле; \\(\\bar{y}_{..}\\) – среднее значение зависимой переменной в родительском узле. Сумма квадратов внутри групп (дочерних узлов) определяется по формуле: \\[SS_{внутригрупп}=\\sum^{B}_{i=1}\\sum^{n_i}_{j=1}n_i(y_{ij}-\\bar{y}_{i.})^2,\\] где \\(y_{ij}\\) – значение зависимой переменной для \\(j\\)-ого наблюдения в \\(i\\)-том дочернем узле; \\(\\bar{y}_{i.}\\) – среднее значение зависимой переменной в \\(i\\)-том дочернем узле. Общая сумма квадратов отклонений имеет вид \\[SS_{общая}=\\sum^{B}_{i=1}\\sum^{n_i}_{j=1}n_i(y_{ij}-\\bar{y}_{..})^2\\] F-критерий – это отношение межгрупповой суммы квадратов отклонений к внутригрупповой: \\[F=frac_{S_{межгрупп}}{S_{внутригрупп}} \\sim F(B-1, n-B)\\] Эта статистика подчиняется F-распределению с \\(B – 1\\) и \\(n – B\\) степенями свободы согласно нулевой гипотезе. p-значение – это вероятность того, что случайная величина с распределением Фишера при условии верности нулевой гипотезы примет значение, не меньшее, чем фактическое значение статистики. Допустим, для нашего примера мы получили значение F-теста 15,943, соответствующее р-значению 0,000. Вероятность того, что F-статистика примет фактическое значение 15,943 и выше, когда средние значения зависимой переменной в категориях предиктора одинаковы, составляет &lt;0,001. Это меньше уровня значимости 0,05. Мы можем отклонить нулевую гипотезу и сделать вывод, что средние значения зависимой переменной в категориях предиктора неодинаковы, а межгрупповые различия являются более существенными, чем внутригрупповые. Данные категории предиктора объединять нельзя. 2.1.4 Способы объединения категорий предикторов Способ объединения категорий предиктора зависит от шкалы его измерения. В номинальных предикторах можно объединять любые категории, если они не различаются значимо по зависимой переменной. Таким образом, для номинальных переменных ограничения на объединение категорий не накладываются. В порядковых предикторах две категории могут быть объединены, только если к ним могут быть присоединены промежуточные категории. Например, переменная, представляющая группы по уровню доходов, может рассматриваться как порядковая. Людей с доходом менее 2000$ имеет смысл объединять с теми, кто зарабатывает более 3000$, только если к вновь образовавшейся группе можно также отнести людей с доходом от 2000$ до 3000$. 2.1.5 Поправка Бонферрони Осуществляя поиск незначимых категорий предиктора для объединения, CHAID выполняет большое количество статистических тестов для различных комбинаций категорий предиктора. Однако число таких комбинаций зависит от количества категорий, которое у каждой переменной разное. Осуществляя поиск незначимых категорий предиктора для объединения, CHAID выполняет большое количество статистических тестов для различных комбинаций категорий предиктора. Однако число таких комбинаций зависит от количества категорий, которое у каждой переменной разное. Например, по одной переменной может оцениваться 2 варианта объединения, рассматриваться 2 таблицы сопряженности и выполняться 2 статистических теста, а по другой переменной – 6 вариантов объединения, 6 таблиц сопряженности и 6 статистических тестов. Вероятность того, что из 6 тестов хи-квадрат для второй переменной по крайней мере один из тестов дает ложное отклонение нулевой гипотезы составляет \\(1-\\prod^{6}_{i=1}(1-\\alpha_i)\\). Групповая вероятность ошибки намного больше индивидуальной вероятности ошибки \\(\\alpha_i\\). Например, если индивидуальная вероятность ошибки (\\(\\alpha_i\\)) по каждому тесту равна 0,05, то групповая вероятность ошибки составит \\(1–0,956 = 0,265\\). Таким образом, при осуществлении множественных проверок гипотез при помощи критерия хи-квадрат (одна проверка на каждое возможное объединение), р-значения недооценивают риск отклонения нулевой гипотезы, когда она верна. Эти рассуждения справедливы и для других статистических критериев. Например, вы можете сделать ошибочный вывод, что заемщики с разными профессиями отличаются по кредитоспособности, тогда как они на самом деле не отличаются. Если мы хотим, чтобы групповая вероятность ошибки при этом не превышала определенный уровень значимости \\(\\alpha\\) (например, 0,05), то, согласно методу Бонферрони, мы должны умножить каждое полученное p-значение на \\(m\\) – количество возможных вариантов объединения с исходных категорий предиктора в \\(g\\) итоговых категорий, получить скорректированное p-значение и сравнить его с уровнем значимости \\(\\alpha\\). Для номинального предиктора множитель \\(m\\) определяется числом Стирлинга второго рода: \\[m=S(c, g) = \\sum^{g-1}_{i=1}\\frac{(-1)^i(g-i)^c}{i!(g-i)!}\\] На рис. 2.2 приводится таблица значений чисел Стирлинга при \\(0≤c\\), \\(g≤9\\). Риc. 2.2. Таблица значений чисел Стерлинга при \\(0≤c\\), \\(g≤9\\) Для порядкового предиктора множитель \\(m\\) определяется как \\[m=\\begin{pmatrix}c-1 \\\\g-1\\end{pmatrix}=\\frac{(c-1)!}{(g-1)!(c-g)!}\\] Допустим, у нас есть три предиктора. Первый – номинальный предиктор Сlass с 8 категориями. Второй – номинальный предиктор Type с 5 категориями. Третий – порядковый предиктор Incomecat c 4 категориями. По завершении этапа объединения Сlass был преобразован в предиктор с тремя итоговыми категориями, и для него было вычислено p-значение 0,00001; Type был преобразован в предиктор с двумя итоговыми категориями, и для него было вычислено p-значение 0,009; Incomecat был преобразован в предиктор с тремя итоговыми категориями и получил p-значение 0,003. Множитель для предиктора Class равен \\(m=S(8, 3)=\\frac{1}{6}(3^8-3\\times2^8+3)=966\\). Таким образом, наше p-значение 0,00001 умножается на 966, и мы получаем скорректированное p-значение 0,00966. Множитель для предиктора Type равен \\(m=S(5, 2)=2^4-1=15\\). Наше p-значение 0,009 умножается на 15, и мы получаем скорректированное p-значение 0,135. Множитель для предиктора Incomecat равен \\(m=\\frac{(4-1)!}{(3-1)!(4-3)!}=\\frac{3!}{2!\\times1!}=\\frac{6}{2}=3\\). p-значение 0,003 умножается на 3, и мы получаем скорректированное p-значение 0,009. Для разбиения узла выбирается предиктор Incomecat, который имеет наименьшее скорректированное p-значение 0,009. Риc. 2.3. Количество возможных разбиений переменной из 4 категорий на 2, 3, 4 группы 2.1.6 XCHAID В 1991 году Дэвид Биггс, Барри Де Вилль и Эд Суен предложили модификацию метода CHAID – XCHAID (от Exhaustive CHAID – исчерпывающий CHAID). Он был разработан для устранения недостатка CHAID – ограниченного набора расщеплений для предиктора. Алгоритм XCHAID приступает к построению дерева, итеративно применяя к каждому узлу, начиная с корневого, процедуры объединения категорий, расщепления узла и проверки правил остановки. Этапы расщепления и остановки в XCHAID аналогичны этапам расщепления и остановки в CHAID. Однако на этапе объединения категорий используется процедура более тщательного поиска категорий: пары категорий продолжают сравниваться и объединяться до тех пор, пока не останется одна пара категорий (напомним, что обычный CHAID прекращает объединение категорий, когда обнаруживает, что все оставшиеся категории статистически значимо различаются между собой). Таким образом, XCHAID позволяет найти наилучшее расщепление для каждого предиктора и затем выбрать, какой предиктор нужно расщепить. Вместе с тем, поскольку объединение категорий осуществляется более тщательно, чем в методе CHAID, XCHAID требует большего времени вычислений. Объединение категорий Для каждого предиктора с числом категорий больше двух алгоритм ищет пару категорий с наименее значимыми различиями по зависимой переменной. Для категориальной зависимой переменной используется критерий хи-квадрат Пирсона. Для количественной зависимой переменной может использоваться F-критерий. Найдя пару с наибольшим p-значением, алгоритм объединяет ее в отдельную составную категорию. Для нового набора категорий предиктора алгоритм вычисляет p-значение. Алгоритм запоминает p-значение и соответствующий набор категорий. Алгоритм повторяет шаги 1, 2, 3 до тех пор, пока не останутся две категории. Затем среди всех наборов категорий предиктора алгоритм находит набор, у которого p-значение на шаге 3 является наименьшим. Алгоритм вычисляет скорректированное p-значение для выбранного набора категорий. Расщепление узла После вычисления скорректированных p-значений для итоговых наборов категорий по всем предикторам алгоритм переходит к этапу расщепления узла. На этапе расщепления алгоритм выбирает, какой предиктор обеспечит наилучшее разбиение узла, то есть имеет наименьшее скорректированное p-значение (наиболее статистически значимый). Найдя предиктор с наименьшим скорректированным p-значением, алгоритм сравнивает его с заданным уровнем значимости для расщепления. Если p-значение: меньше или равно заданному уровню значимости для расщепления – алгоритм разбивает узел с использованием данного предиктора; больше заданного уровня значимости для расщепления, то алгоритм не расщепляет узел и узел рассматривается как терминальный. Остановка Алгоритм проверяет, нужно ли прекратить построение дерева, в соответствии со следующими правилами остановки. Если узел стал однородным, то есть все наблюдения в узле имеют одинаковые значения зависимой переменной, узел не разбивается. Если текущая глубина дерева достигает заданной пользователем максимальной глубины дерева, процесс построения дерева останавливается. Если количество наблюдений в родительском узле меньше заданного пользователем минимума наблюдений в родительском узле, узел не разбивается. Если минимальное абсолютное количество наблюдений в терминальном узле меньше заданного пользователем минимума наблюдений в терминальном узле, узел не разбивается. Если минимальная относительная частота наблюдений в терминальном узле меньше заданной пользователем минимальной относительной частоты наблюдений в терминальном узле, узел не разбивается. На практике метод XCHAID строит модель с одинаковой или чуть лучшей дискриминирующей способностью, чем метод CHAID, однако эта разница, как правило, не является статистически значимой. Если учесть, что при одинаковом качестве моделей времени на подгонку в случае использования метода XCHAID требуется больше (особенно это актуально при работе с большими выборками), данный метод используется редко. 2.1.7 Иллюстрация работы CHAID на конкретном примере Предположим, есть данные по клиентам микрофинансовой организации и известно, выплатили они займ или нет (категориальная зависимая переменная Просрочка). В качестве потенциальных предикторов фигурируют четыре переменных: Доход, Возраст, Сфера занятости, Пол. Переменные Пол и Сфера занятости являются номинальными, переменные Доход и Возраст – порядковыми. Переменная Сфера занятости принимает значения “Работает по найму”, “Свое дело”, “Учится или студент”, “Пенсионер”. Переменная Доход принимает значения “Менее 10 тыс. рублей”, “От 10 до 25 тыс. рублей”, “От 26 до 40 тыс. рублей”, “Более 40 тыс. рублей”. Переменная Пол принимает значения “Женщина” и Мужчина. Переменная Возраст принимает значения “&lt;24”, “24-26”, “27-28”, “29-31”, “32-33”, “34-35”, “36-38”, “39-41”, “42-46” и “&gt;46”. Необходимо выяснить, какие группы клиентов с большей вероятностью выйдут в просрочку, чтобы сосредоточить внимание на них. Схематично наши исходные данные представлены на рис. 2.4. Риc. 2.4. Исходные данные перед началом работы CHAID Что же делает CHAID, когда мы запускаем его? По каждому предиктору CHAID берет пару категорий, сравнивает, различаются ли они по зависимой переменной, и объединяет их, если они не показывают этого различия (дают p-значение больше заданного уровня значимости для объединения). В нашем случае по порядковому предиктору Доход CHAID сравнивает категорию “Менее 10 тыс. рублей” с категорией “От 10 до 25 тыс. рублей”, затем категорию “От 10 до 25 тыс. рублей” с категорией “От 26 до 40 тыс. рублей”, затем категорию “От 26 до 40 тыс. рублей” с категорией \"Более 40 тыс. рублей“. Еще раз обратите внимание, что в порядковом предикторе несмежные категории (например, категория ”Менее 10 тыс. рублей\" и категория “Более 40 тыс. рублей”) сравниваться и объединяться не могут. Допустим, категории предиктора Доход “Менее 10 тыс. рублей” и “От 10 до 25 тыс. рублей” значимо не различаются по кредитоспособности (в обоих категориях наблюдается высокая доля «плохих» заемщиков), имеют наибольшее p-значение. Тогда CHAID объединяет их и формирует новый набор категорий (объединенная категория “Менее 10 тыс. рублей/От 10 до 25 тыс. рублей”, категория “От 26 до 40 тыс. рублей”, категория “Более 40 тыс. рублей”) и снова начинает процесс сравнения. Процесс объединения категорий остановится, когда все оставшиеся категории предиктора будут различаться на заданном уровне значимости для объединения. Сформировав новый набор категорий по предиктору Доход, алгоритм начинает аналогичным образом формировать набор категорий для порядкового предиктора Возраст, опять же сравнивая и объединяя только смежные категории. Затем алгоритм формирует набор категорий для номинального предиктора Сфера занятости. Здесь уже CHAID может сравнивать и объединять любые категории переменной. Затем переходит к предиктору Пол. Здесь категории предиктора Пол не могут быть объединены, поскольку у этой переменной только два уровня. Процесс сравнения и объединения категорий по каждому предиктору показан на рис. 2.5. Риc. 2.5. Объединение категорий предикторов В итоге получаем преобразованные предикторы Доход (допустим, набор из 3 категорий: объединенная категория “Менее 10 тыс. рублей/От 10 до 25 тыс. рублей”, категория “От 26 до 40 тыс. рублей”, категория “Более 40 тыс. рублей”), Сфера занятости (набор из 3 категорий: объединенная категория “Работает по найму/Свое дело”, категория “Учится или студент”, категория “Пенсионер”), Возраст (набор из 3 категорий: объединенная категория “Менее 29 лет”, объединенная категория “От 29 до 46 лет”, категория “Старше 46 лет”) и предиктор Пол (категория “Женщины” и категория “Мужчины”). Завершив этап объединения, CHAID переходит к этапу разбиения узла (рис. 2.6). Здесь происходит вычисление p-значений для итоговых наборов категорий, затем p-значения корректируются с помощью поправки Бонферрони, чтобы учесть количество сравнений категорий по каждому предиктору. Риc. 2.6. Выбор предиктора для разбиения узла Например, лучшим предиктором объявлена переменная Доход (имеет наименьшее скорректированное p-значение, не превышающее заданный уровень значимости для разбиения узла). Тогда CHAID обращается к первой новой группе (например, объединенной категории “Менее 10 тыс. рублей/От 10 до 25 тыс. рублей”) и снова повторяет вышеописанные шаги, ищет наименее различающиеся категории для объединения и выбирает наиболее значимый предиктор для разбиения. Предположим, что для рассматриваемой группы таким предиктором стал Пол. Тогда CHAID разделяет группу “Менее 10 тыс. рублей/От 10 до 25 тыс. рублей” по переменной Пол. Затем он исследует каждую из оставшихся групп, образованную переменной Доход (категории “От 26 до 40 тыс. рублей”, “Более 40 тыс. рублей”), снова по каждой группе проверяет категории предикторов на объединение и разбивает узел с помощью предиктора, который наиболее значимо связан с зависимой переменной для этой группы. Затем CHAID опускается на следующий уровень дерева и берет первую группу предиктора Пол “Женщины” внутри группы “Менее 10 тыс. рублей/От 10 до 25 тыс. рублей”, снова исследует категории и выясняет, есть ли среди предикторов значимо влияющие на зависимую переменную. Если таких предикторов для группы “Женщины” внутри группы “Менее 10 тыс. рублей/От 10 до 25 тыс. рублей” не оказывается или выполняется условие остановки, то CHAID объявляет эту группу терминальным узлом и переходит к аналогичному исследованию группы “Мужчины” внутри группы “Менее 10 тыс. рублей/От 10 до 25 тыс. рублей”. Таким способом, уровень за уровнем, CHAID систематически разделяет данные на группы (называемые узлами), показывающие значимые различия по отношению к зависимой переменной. Результаты этого процесса представляются в форме дерева, в котором ветвление происходит по мере деления на группы. Взглянув на построенное дерево (рис. 2.7), мы можем с уверенностью ответить на ряд вопросов. Какие из предикторов взаимосвязаны с переменной дефолта, помогают предсказать ее? Какие комбинации категорий этих предикторов дают наибольший процент попадания в интересующую категорию зависимой переменной? Они представляют собой целевые группы, на которых нужно сосредоточить внимание. В следующих разделах будет подробно рассказано, как строить и интерпретировать дерево решений CHAID в пакете R CHAID. Риc. 2.7. Итоговое дерево CHAID 2.2 Предварительная подготовка данных перед построением модели дерева CHAID 2.2.1 Загрузка данных Данные, которыми мы воспользуемся для построения дерева классификации CHAID, записаны в файле Churn.csv. Исходная выборка содержит записи о 4431 клиенте, классифицированном на два класса: 0 — оттока нет (2496 клиентов) и 1 — отток есть (1935 клиентов). По каждому наблюдению (клиенту) фиксируются следующие переменные (характеристики): порядковый предиктор Длительность междугородних звонков в минутах [longdist]; порядковый предиктор Длительность местных звонков в минутах [local]; номинальный предиктор Наличие скидки на междугородние звонки [int_disc]; номинальный предиктор Тип местных звонков [billtype]; номинальный предиктор Способ оплаты [pay]; номинальный предиктор Пол [gender]; номинальный предиктор Семейное положение [marital]; количественный предиктор Доход [income]; порядковый предиктор Возрастная категория [agecat]; номинальная зависимая переменная Наличие оттока [churn]. Необходимо разработать модель оттока, с помощью которой предполагается классифицировать новых клиентов на лояльных и склонных к уходу. Запустим R. Для работы нам потребуются следующие пакеты: dplyr; Hmisc; stringr; car; imputeMissings; lsr; CHAID; pROC; precrec; xlsx. Давайте установим их с помощью функции install.packages(). Если вы используете консольную версию R, вам будет предложено выбрать постоянный CRAN-репозиторий, из которого будут устанавливаться пакеты. install.packages(&quot;data.table&quot;) install.packages(&quot;Hmisc&quot;) install.packages(&quot;stringr&quot;) install.packages(&quot;car&quot;) install.packages(&quot;imputeMissings&quot;) install.packages(&quot;lsr&quot;) install.packages(&quot;CHAID&quot;, repos = &quot;http://R-Forge.R-project.org&quot;) install.packages(&quot;pROC&quot;) install.packages(&quot;precrec&quot;) install.packages(&quot;xlsx&quot;) Иногда при установке пакетов появляется ошибка: *dependencies название_пакета are not available for package название_ устанавливаемого _пакета*. Это говорит о том, что перед установкой основного пакета вы должны установить требуемые пакеты-зависимости. После установки пакетов их можно загрузить для работы с помощью функции library(): library(data.table) ## data.table 1.13.0 using 4 threads (see ?getDTthreads). Latest news: r-datatable.com library(Hmisc) ## Loading required package: lattice ## Loading required package: survival ## Loading required package: Formula ## Loading required package: ggplot2 ## Want to understand how all the pieces fit together? Read R for Data Science: ## https://r4ds.had.co.nz/ ## ## Attaching package: &#39;Hmisc&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## format.pval, units library(stringr) library(car) ## Loading required package: carData library(imputeMissings) ## ## Attaching package: &#39;imputeMissings&#39; ## The following object is masked from &#39;package:Hmisc&#39;: ## ## impute library(lsr) library(CHAID) ## Loading required package: partykit ## Loading required package: grid ## Loading required package: libcoin ## Loading required package: mvtnorm library(pROC) ## Type &#39;citation(&quot;pROC&quot;)&#39; for a citation. ## ## Attaching package: &#39;pROC&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## cov, smooth, var library(precrec) ## ## Attaching package: &#39;precrec&#39; ## The following object is masked from &#39;package:pROC&#39;: ## ## auc library(readxl) При помощи функции data.table::fread() загрузим файл Churn.csv в таблицу данных dt: dt &lt;- fread(&quot;data/Churn.csv&quot;, na.strings = &quot;&quot;) Если предиктор имеет одну категорию, он исключается из анализа. Если предиктор имеет две категории, происходит переход к шагу 3.↩︎ "],
["ch3.html", "3 Построение деревьев решений CART с помощью пакета R rpart 3.1 Знакомство с методом CART", " 3 Построение деревьев решений CART с помощью пакета R rpart 3.1 Знакомство с методом CART Алгоритм метода CART был опубликован в 1984 году в книге «Classification and regression trees» («Деревья классификации и регрессии»), авторами которой являются Лео Брейман (Калифорнийский университет в Беркли), Джером Фридман (Стэнфордский университет), Ричард Олшен (Калифорнийский университет в Беркли) и Чарльз Стоун (Стэнфордский университет). Как уже ясно из названия, CART включает два вида анализа: деревья классификации для категориальных зависимых переменных и регрессионные деревья для количественных зависимых переменных. Эти виды анализа отличаются в деталях, но используют общий принцип. CART выполняет последовательные бинарные разбиения данных на основе выбранного критерия. В отличие от CHAID, в CART для выбора предиктора расщепления не применяются статистические критерии. Вместо этого в каждом узле при расщеплении данных используется предиктор, обеспечивающий наибольшее улучшение по выбранному критерию. Ключевой элемент в методе CART – это отсечение ветвей дерева, известный под названием «отсечение с минимизацией стоимости-сложности» (minimal cost-complexity pruning). Деревья, построенные c помощью метода CART, имеют тенденцию быть слишком большими, а результаты не воспроизводятся с необходимой степенью устойчивости. Авторы метода пришли к выводу, что если разрешить построение максимально большого дерева, но затем отсечь его ветви, используя более сложный критерий, то в результате будет построено меньшее дерево лучшего качества. Построение дерева с последующим отсечением его ветвей стало основой метода CART. 3.1.1 Описание алгоритма Алгоритм CART строит дерево, итеративно применяя к каждому узлу, начиная с корневого, процедуры выбора наилучшего расщепления предиктора, выбора наилучшего расщепления узла и остановки. В качестве критерия расщепления алгоритм использует уменьшение неоднородности при разбиении родительского узла на дочерние узлы (см. раздел 3.1.2. Неоднородность). Этап 1. Выбор наилучшего расщепления предиктора 1. Алгоритм начинает с поиска наилучшей точки расщепления (разделяющего значения) для каждого предиктора. Для количественных и порядковых предикторов алгоритм выполняет сортировку значений в порядке возрастания. Затем алгоритм разбивает предиктор по всем возможным точкам расщепления (разделяющим значениям). Если имеются 6 различных значений возраста, они будут упорядочены и для них будет созданы 5 точек расщепления. Например, есть значения возраста 18, 35, 20, 16, 11, 10. Они будут упорядочены: 10, 11, 16, 18, 20 и 35. Будет рассмотрено пять расщепляющих значений: &lt; 11 s ≥ 11 &lt; 16 s ≥ 16 &lt; 18 s ≥ 18 &lt; 20 s ≥ 20 &lt; 35 s ≥ 35 Именно эта стратегия используется в пакете R rpart. Для номинального предиктора его категории делятся всеми возможными способами на две группы. В каждой возможной точке расщепления переменной вся выборка наблюдений гипотетически разбивается на два дочерних узла: левый и правый. Все наблюдения, у которых значение предиктора меньше точки расщепления, относятся в левый дочерний узел. Все наблюдения, у которых значение предиктора больше или равно точке расщепления, относятся в правый дочерний узел. 2. Алгоритм вычисляет уменьшение неоднородности для каждой точки расщепления. 3. В качестве наилучшей точки расщепления предиктора алгоритма выбирается точка расщепления, дающая наибольшее уменьшение неоднородности при разбиении родительского узла на дочерние узлы. Вышеописанные шаги повторяются для всех остальных переменных. Этап 2. Выбор наилучшего расщепления узла 1. Из наилучших точек расщеплений предикторов, полученных на первом этапе, алгоритм выбирает точку расщепления, максимизирующую уменьшение неоднородности (проще говоря, из лучших расщеплений для каждого предиктоа выбирается наилучшее). 2. Алгоритм расщепляет узел, используя найденную для него наилучшую точку расщепления, если это позволяют правила остановки. Обратите внимание, что каждый предиктор может неоднократно использоваться для расщепления в ветви дерева. Например, может быть выполнено расщепление по переменной Возраст в значении 60 лет, а затем в узле-потомке снова может быть выполнено расщепление по этой переменной. Таким образом, могут моделироваться сложные зависимости между непрерывным предиктором и зависимой переменной, несмотря на то, что выполняются только бинарные расщепления. Этап 3. Остановка Алгоритм проверяет, нужно ли прекратить построение дерева, в соответствии со следующими правилами остановки. 1. Если узел стал однородным, то есть все наблюдения в узле имеют одинаковые значения зависимой переменной, узел не разбивается. 2. Если при выполнении разбиения уменьшение ошибки модели становится меньше порогового значения штрафа за сложность, процесс построения дерева останавливается. 3. Если количество наблюдений в родительском узле меньше заданного пользователем минимума наблюдений в родительском узле, узел не разбивается. 4. Если минимальное количество наблюдений в терминальном узле меньше заданного пользователем минимума наблюдений в терминальном узле, узел не разбивается. 5. Если текущая глубина дерева достигает заданной пользователем максимальной глубины дерева, процесс построения дерева останавливается. ПРИМЕЧАНИЕ В пакете R rpart с помощью ряда параметров вспомогательной функции rpart.control можно изменить некоторые вышеперечисленные правила остановки: cp задает штраф за сложность: если разбиение уменьшает ошибку модели на значение, меньшее порогового значения cp, оно не принимается и дерево останавливается в росте; minsplit задает минимальное количество наблюдений в родительском узле перед расщеплением, по умолчанию 20; minbucket задает минимальное количество наблюдений в терминальном узле, по умолчанию используется округленное значение minsplit/3; maxdepth задает максимальную глубину дерева (количество уровней дерева, лежащих ниже корневого узла), по умолчанию равна 30. 3.1.2 Неоднородность При построении дерева CART расщепляет узел на два дочерних узла по предиктору, который обеспечивает наибольшее уменьшение неоднородности. Для этого неоднородность родительского узла сравнивается со взвешенным средним значением неоднородностей дочерних узлов: \\[\\Delta i=i_{P}-\\left(\\frac{n_{L}}{n_{P}}i_{L} + \\frac{n_{R}}{n_{P}}i_{R}\\right),\\] где \\(\\Delta i\\) – уменьшение неоднородности; \\(i_{p}\\) – неоднородность родительского узла; \\(\\left(\\frac{n_{L}}{n_{P}}i_{L} + \\frac{n_{R}}{n_{P}}i_{R}\\right)\\) – взвешенное среднее значение неоднородностей дочерних узлов: \\(n_{L}\\) – количество наблюдений в левом дочернем узле; \\(n_{R}\\) – количество наблюдений в правом дочернем узле; \\(n_{P}\\) – количество наблюдений в родительском узле; \\(i_{L}\\) – неоднородность левого дочернего узла; \\(i_{R}\\) – неоднородность правого дочернего узла. Применительно к дереву классификации CART под неоднородностью понимается неоднородность распределения классов зависимой переменной в узле. Однородным узлом является тот, в котором все наблюдения относятся к одному и тому же классу зависимой переменной, в то время как узел с максимальной неоднородностью содержит равное количество наблюдений во всех классах зависимой переменной. Допустим, есть узел, содержащий 6 наблюдений, относящимся к одному из двух классов. Максимальная неоднородность в узле будет достигнута при разбиении его на два класса по 3 наблюдения в каждом, а минимальная неоднородность – при разбиении на 6 наблюдений одного класса и 0 наблюдений другого класса. Рис. 3.1. Примеры, иллюстрирующие минимальную и максимальную неоднородность Наиболее популярная мера неоднородности для деревьев классификации – мера Джини. В основе меры Джини лежат возведенные в квадрат вероятности, с которыми наблюдения будут отнесены к каждому классу зависимой переменной. Общая формула для вычисления меры Джини выглядит так: \\[Gini(t)=1-\\sum^{K}_{k=1}p^{2}_{k},\\] где \\(K\\) – количество классов зависимой переменной; \\(k\\) – класс зависимой переменной; \\(p_{k}\\) – вероятность \\(k\\)-того класса зависимой переменной в \\(t\\)-ом узле. Для бинарной зависимой переменной мера Джини принимает вид: \\[Gini(t)=1-p^{2}_{1}-p^{2}_{0},\\] где \\(p^{2}_{1}\\) – вероятность класса 1 (положительного класса) в \\(t\\)-ом узле; \\(p^{2}_{0}\\) – вероятность класса 0 (отрицательного класса) в \\(t\\)-ом узле. Когда наблюдения в узле равномерно распределены по категориям, мера Джини принимает свое максимальное значение (для бинарной зависимой переменной максимальное значение меры Джини равно 0,5). Когда все наблюдения в узле принадлежат к одному и тому же классу, мера Джини равна 0. Распределение в узлах Мера Джини Узел с распределением (1, 0) Мера Джини = 1 – 12 – 02 = 0 Узел с распределением (0,5, 0,5) Мера Джини = 1 – 0,52 – 0,52 = 0,5 Узел с распределением (0,7, 0,3) Мера Джини = 1 – 0,72 – 0,32 = 0,42 Кроме меры Джини часто используется энтропия. Она вычисляется по формуле: \\[E(t)=-\\sum^{K}_{k=1}p_{k}\\times log_{2}p_{k}\\] Обе меры показаны на рис. 3.2, где можно четко увидеть, что энтропия (мера Джини) минимальна, когда все наблюдения либо принадлежат отрицательному классу, либо принадлежат положительному классу, и максимальна в случае одинакового количества наблюдений каждого класса. Рис. 3.2. Сравнение меры Джини и энтропии для двух классов Применительно к дереву регрессии CART под неоднородностью понимается степень разброса значений количественной зависимой переменной вокруг среднего значения в узле. Более точно, речь идет о среднеквадратичной ошибке – сумме квадратов остатков (разностей между фактическими значениями зависимой переменной и ее средним значением) в конкретном узле, поделенной на количество наблюдений в этом узле. \\[MSE(t)=\\frac{1}{n_{t}} \\sum_{i}(y_{it}- \\bar{y_{t}})^2,\\] где \\(y_{it}\\) – фактическое значение зависимой переменной для \\(i\\)-того наблюдения в t-ом узле; \\(\\bar{y_{t}}\\) – среднее значение зависимой переменной для в \\(t\\)-ом узле; \\(n_{t}\\) – количество наблюдений в t-ом узле. 3.1.3 Метод отсечения ветвей на основе меры стоимости-сложности с перекрестной проверкой При использовании критерия неоднородности для построения дерева возникает следующая проблема. Увеличивая размеры дерева, почти всегда можно уменьшить неоднородность. Любое дерево будет иметь нулевую неоднородность, если оно построено достаточно большим. В частности, если в каждом терминальном узле имеется только одно наблюдение, то неоднородность равна нулю. По мере увеличения размеров дерево становится более сложным, а ошибка модели на обучающей выборке уменьшается вплоть до 0, когда каждое наблюдение находится в отдельном узле. Но такая ситуация чревата переобучением, когда на новых данных качество модели будет гораздо хуже полученного на обучающей выборке. В итоге, несмотря на высокий процент правильных прогнозов на обучающей выборке, большие деревья могут обладать плохой обобщающей способностью. Поэтому необходимо найти баланс между сложностью и ошибкой модели. Для решения этих проблем разработчики CART ввели меру стоимости-сложности, которая включает штраф, возрастающий с увеличением размера дерева. Эта функция для дерева (или его ветви) обычно выражается как \\(R_{\\alpha}=R(T)-\\alpha \\lvert T \\rvert\\), где \\(R(T)\\) – ошибка модели, рассчитанная по тем же данным, по которым строилось дерево; \\(\\alpha\\) – коэффициент штрафа; \\(\\lvert T \\rvert\\) – количество терминальных узлов дерева (или ветви) \\(T\\). Дерево большего размера будет иметь большую меру стоимости-сложности за счет слагаемого \\(\\alpha \\lvert T \\rvert\\). Для того, чтобы мера стоимости-сложности улучшилась, ошибка модели («стоимость» ошибки) должна уменьшиться на величину, большую, чем штраф за сложность (в пакете R rpart штраф за сложность регулируется гиперпараметром cp). Мера стоимости-сложности была протестирована в качестве критерия построения дерева, однако авторы (Брейман, Фридман и др.) констатировали, что построенные таким способом деревья все еще не вполне удовлетворительны – они недостаточно стабильны. Решение этой проблемы привело, в свою очередь, к методу отсечения ветвей на основе критерия максимального уменьшения меры стоимости-сложности (cost-complexity pruning). Его суть сводится к следующему. Сначала строим максимально большое дерево (с небольшим числом наблюдений в узлах – от 1 до 5). Затем отсекаем у него ветви на основе меры стоимости-сложности. Выбираем простейшее дерево с наименьшим числом узлов, ошибка которого находится в пределах одной стандартной ошибки от минимальной ошибки, достигнутой на этапе построения дерева. В качестве ошибки для дерева классификации берется ошибка классификации, а для дерева регрессии таковой будет среднеквадратичная ошибка. В дальнейшем этот метод был вновь усовершенствован авторами. В ходе экспериментов было установлено, что управление отсечением и отбор модели необходимо осуществлять, оценивая качество модели не на обучающей выборке, а на контрольных блоках перекрестной проверки (Лео Брейман рекомендовал использовать 10-блочную перекрестную проверку). Мы выбираем дерево, которое дает наименьшую кросс-валидационную ошибку. Кросс-валидационной ошибкой является ошибка классификации (для дерева классификации) или среднеквадратичная ошибка (для дерева регрессии), усредненная по всем контрольным блокам перекрестной проверки. Метод отсечения на основе меры стоимости-сложности с перекрестной проверкой как раз и реализован в пакете rpart. 3.1.4 Обработка пропущенных значений В методе CART пропущенные значения обрабатываются с использованием переменных-суррогатов. Таким образом, если наблюдение имеет пропущенное значение в переменной, по которой осуществляется разбиение узла, то для выбора дочернего узла, к которому относится данное наблюдение, используется его значение для наилучшей переменной-суррогата. Наилучшей переменной-суррогатом является альтернативная предикторная переменная, дающая наиболее близкое (с использованием меры связи) разбиение к тому, которое дает исходный предиктор. 3.1.5 Иллюстрация работы метода CART на конкретных примерах 3.1.5.1 Дерево классификации "]
]
