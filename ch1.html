<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>1 Введение в метод деревьев решений | Деревянные алгоритмы в R и Python</title>
  <meta name="description" content="1 Введение в метод деревьев решений | Деревянные алгоритмы в R и Python." />
  <meta name="generator" content="bookdown 0.20 and GitBook 2.6.7" />

  <meta property="og:title" content="1 Введение в метод деревьев решений | Деревянные алгоритмы в R и Python" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="1 Введение в метод деревьев решений | Деревянные алгоритмы в R и Python." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="1 Введение в метод деревьев решений | Деревянные алгоритмы в R и Python" />
  
  <meta name="twitter:description" content="1 Введение в метод деревьев решений | Деревянные алгоритмы в R и Python." />
  

<meta name="author" content="Артем Груздев, Андрей Огурцов" />


<meta name="date" content="2020-08-01" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="ch2.html"/>
<script src="libs/header-attrs-2.3/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Вступление</a></li>
<li class="chapter" data-level="1" data-path="ch1.html"><a href="ch1.html"><i class="fa fa-check"></i><b>1</b> Введение в метод деревьев решений</a>
<ul>
<li class="chapter" data-level="1.1" data-path="ch1.html"><a href="ch1.html#ch1.1"><i class="fa fa-check"></i><b>1.1</b> Введение в методологию деревьев решений</a></li>
<li class="chapter" data-level="1.2" data-path="ch1.html"><a href="ch1.html#ch1.2"><i class="fa fa-check"></i><b>1.2</b> Краткий обзор методов деревьев решений CHAID и CART</a></li>
<li class="chapter" data-level="1.3" data-path="ch1.html"><a href="ch1.html#ch1.3"><i class="fa fa-check"></i><b>1.3</b> Преимущества и недостатки деревьев решений</a></li>
<li class="chapter" data-level="1.4" data-path="ch1.html"><a href="ch1.html#ch1.4"><i class="fa fa-check"></i><b>1.4</b> Задачи, выполняемые с помощью деревьев решений</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ch2.html"><a href="ch2.html"><i class="fa fa-check"></i><b>2</b> Построение деревьев решений CHAID с помощью пакета R CHAID</a>
<ul>
<li class="chapter" data-level="2.1" data-path="ch2.html"><a href="ch2.html#ch2.1"><i class="fa fa-check"></i><b>2.1</b> Знакомство с методом CHAID</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="ch2.html"><a href="ch2.html#ch2.1.1"><i class="fa fa-check"></i><b>2.1.1</b> Описание алгоритма</a></li>
<li class="chapter" data-level="2.1.2" data-path="ch2.html"><a href="ch2.html#ch2.1.2"><i class="fa fa-check"></i><b>2.1.2</b> Немного о критерии хи-квадрат</a></li>
<li class="chapter" data-level="2.1.3" data-path="ch2.html"><a href="ch2.html#ch2.1.3"><i class="fa fa-check"></i><b>2.1.3</b> Немного об F-критерии</a></li>
<li class="chapter" data-level="2.1.4" data-path="ch2.html"><a href="ch2.html#ch2.1.4"><i class="fa fa-check"></i><b>2.1.4</b> Способы объединения категорий предикторов</a></li>
<li class="chapter" data-level="2.1.5" data-path="ch2.html"><a href="ch2.html#ch2.1.5"><i class="fa fa-check"></i><b>2.1.5</b> Поправка Бонферрони</a></li>
<li class="chapter" data-level="2.1.6" data-path="ch2.html"><a href="ch2.html#ch2.1.6"><i class="fa fa-check"></i><b>2.1.6</b> XCHAID</a></li>
<li class="chapter" data-level="2.1.7" data-path="ch2.html"><a href="ch2.html#ch2.1.7"><i class="fa fa-check"></i><b>2.1.7</b> Иллюстрация работы CHAID на конкретном примере</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="ch2.html"><a href="ch2.html#ch2.2"><i class="fa fa-check"></i><b>2.2</b> Предварительная подготовка данных перед построением модели дерева CHAID</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="ch2.html"><a href="ch2.html#ch2.2.1"><i class="fa fa-check"></i><b>2.2.1</b> Загрузка данных</a></li>
<li class="chapter" data-level="2.2.2" data-path="ch2.html"><a href="ch2.html#ch2.2.2"><i class="fa fa-check"></i><b>2.2.2</b> Фиксация пустых строковых значений как пропусков</a></li>
<li class="chapter" data-level="2.2.3" data-path="ch2.html"><a href="ch2.html#ch2.2.3"><i class="fa fa-check"></i><b>2.2.3</b> Вывод подробной информации о переменных</a></li>
<li class="chapter" data-level="2.2.4" data-path="ch2.html"><a href="ch2.html#ch2.2.4"><i class="fa fa-check"></i><b>2.2.4</b> Нормализация строковых значений</a></li>
<li class="chapter" data-level="2.2.5" data-path="ch2.html"><a href="ch2.html#ch2.2.5"><i class="fa fa-check"></i><b>2.2.5</b> Обработка дублирующихся наблюдений</a></li>
<li class="chapter" data-level="2.2.6" data-path="ch2.html"><a href="ch2.html#ch2.2.6"><i class="fa fa-check"></i><b>2.2.6</b> Изменение типов переменных</a></li>
<li class="chapter" data-level="2.2.7" data-path="ch2.html"><a href="ch2.html#ch2.2.7"><i class="fa fa-check"></i><b>2.2.7</b> Обработка редких категорий</a></li>
<li class="chapter" data-level="2.2.8" data-path="ch2.html"><a href="ch2.html#ch2.2.8"><i class="fa fa-check"></i><b>2.2.8</b> Однократное случайное разбиение набора данных на обучающую и контрольную выборки для проверки модели</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="ch3.html"><a href="ch3.html"><i class="fa fa-check"></i><b>3</b> Построение деревьев решений CART с помощью пакета R rpart</a>
<ul>
<li class="chapter" data-level="3.1" data-path="ch3.html"><a href="ch3.html#ch3.1"><i class="fa fa-check"></i><b>3.1</b> Знакомство с методом CART</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="ch3.html"><a href="ch3.html#ch3.1.1"><i class="fa fa-check"></i><b>3.1.1</b> Описание алгоритма</a></li>
<li class="chapter" data-level="3.1.2" data-path="ch3.html"><a href="ch3.html#ch3.1.2"><i class="fa fa-check"></i><b>3.1.2</b> Неоднородность</a></li>
<li class="chapter" data-level="3.1.3" data-path="ch3.html"><a href="ch3.html#ch3.1.3"><i class="fa fa-check"></i><b>3.1.3</b> Метод отсечения ветвей на основе меры стоимости-сложности с перекрестной проверкой</a></li>
<li class="chapter" data-level="3.1.4" data-path="ch3.html"><a href="ch3.html#ch3.1.4"><i class="fa fa-check"></i><b>3.1.4</b> Обработка пропущенных значений</a></li>
<li class="chapter" data-level="3.1.5" data-path="ch3.html"><a href="ch3.html#ch3.1.5"><i class="fa fa-check"></i><b>3.1.5</b> Иллюстрация работы метода CART на конкретных примерах</a></li>
</ul></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Деревянные алгоритмы в R и Python</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ch1" class="section level1" number="1" style="text-align: justify">
<h1><span class="header-section-number">1</span> Введение в метод деревьев решений</h1>
<div id="ch1.1" class="section level2" number="1.1">
<h2><span class="header-section-number">1.1</span> Введение в методологию деревьев решений</h2>
<p>Как и регрессионный анализ, деревья решений являются методом изучения статистической взаимосвязи между одной зависимой переменной и несколькими независимыми (предикторными) переменными. При этом под зависимой переменной понимается переменная, «поведение» которой мы хотим предсказать, а под предикторами подразумеваются переменные, которые помогают нам это сделать<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>. Базовое отличие метода деревьев решений от регрессионного анализа заключается в том, что взаимосвязь между значением зависимой переменной и значениями независимых переменных представлена не в виде общего прогнозного уравнения, а в виде древовидной структуры, которую получают с помощью иерархической сегментации данных.</p>
<p>Берется весь обучающий набор данных, называемый <strong>корневым узлом</strong>, и разбивается на два или более <strong>узлов</strong> (<strong>сегментов</strong>) так, чтобы наблюдения, попавшие в разные узлы, максимально отличались друг от друга по зависимой переменной (например, выделяем два узла с наибольшим и наименьшим процентом «плохих» заемщиков). В роли <strong>правил разбиения</strong>, максимизирующих эти различия, выступают значения независимых переменных (пол, возраст, доход и др.). Качество разбиения оценивается с помощью статистических критериев. Обычно оценка качества разбиения происходит в два этапа. На первом этапе по каждому предиктору осуществляется наилучшее правило разбиения, а на втором этапе из наилучших правил, найденных по каждому предиктору на первом этапе, выбирается самое лучшее. Правила отмечаются на <strong>ветвях</strong> – линиях, которые соединяют разбиваемый узел с узлами, полученными в результате разбиения. Для каждого узла вычисляются <strong>вероятности</strong> в виде <strong>процентных долей</strong> категорий зависимой переменной (если зависимая переменная является категориальной) или средние значения зависимой переменной (если зависимая переменная является количественной). В результате выносится <strong>решение</strong> – спрогнозированная категория зависимой переменной (если зависимая переменная является категориальной) или спрогнозированное среднее значение зависимой переменной (если зависимая переменная является количественной).</p>
<p>Аналогичным образом, каждый узел, получившийся в результате разбиения корневого узла, разбивается дальше на узлы, т.е. узлы внутри узла, и т.д. Этот процесс продолжается до тех пор, пока есть возможность создания новых узлов. Данный процесс сегментации называется <strong>рекурсивным разделением</strong>. Получившаяся иерархическая структура, характеризующая взаимосвязь между значением зависимой переменной и значениями независимых переменных, называется <strong>деревом</strong>.</p>
<p>Иногда для обозначения разбиваемого узла применяется термин <strong>родительский узел</strong>. Новые узлы, получившиеся в результате разбиения, называются <strong>дочерними узлами</strong> (или <strong>узлами-потомками</strong>). Когда впоследствии дочерний узел разбивается сам, он становится родительским узлом. Окончательные узлы, которые в дальнейшем не разбиваются, называются <strong>терминальными узлами</strong> (<strong>листьями</strong>) дерева. Лист представляет собой наилучшее окончательное решение, выдаваемое деревом. Здесь мы определяем группы клиентов, обладающие желаемыми характеристиками (например, тех, кто погасит кредит или откликнется на наше маркетинговое предложение).</p>
<p>В случае, когда вы прогнозируете вероятность значения категориальной зависимой переменной по соответствующим значениям предикторов, дерево решений называют <strong>деревом классификации</strong> (рис. 1.1). Например, дерево классификации строится для вычисления вероятности отклика у заемщика (на основе спрогнозированной вероятности мы относим его к неоткликнувшемуся – к классу 0 или к откликнувшемуся – классу 1). Если дерево решений используется для того, чтобы спрогнозировать среднее значение количественной зависимой переменной по соответствующим значениям предикторов, его называют <strong>деревом регрессии</strong> (рис. 1.2). Например, дерево регрессии строится, чтобы вычислить средний размер вклада у клиента.</p>
<p><img src="figures/1.1.PNG" /></p>
<p><em>Рис. 1.1. Дерево классификации (на примере пакета R <code>rpart</code>)</em></p>
<p><img src="figures/1.2.PNG" /></p>
<p><em>Рис. 1.2. Дерево регрессии (на примере пакета R <code>rpart</code>)</em></p>
<p>Если визуализировать работу алгоритма дерева решений, то мы увидим, что алгоритм последовательно разбивает данные на прямоугольники, параллельные осям координат. Проиллюстрируем это на примере бинарного дерева решений, то есть случая, когда узел-родитель может иметь только два узла-потомка.</p>
<p>У нас есть набор данных, состоящий из 32 наблюдений, предикторами являются переменные <em>Длительность звонков в минутах</em> и <em>Количество обращений в службу поддержки</em>, зависимая переменная – <em>Статус клиента</em>: 18 клиентов относятся к классу <em>Ушедший</em>, а 14 – к классу <em>Оставшийся</em>.</p>
<p><img src="figures/1.3.PNG" /></p>
<p><em>Рис. 1.3. Визуальное представление набора данных</em></p>
<p>Первое разбиение набора данных происходит по предиктору <em>Количество обращений в службу поддержки</em>.</p>
<p><img src="figures/1.4.PNG" /></p>
<p><em>Рис. 1.4. Разбиение набора данных по предиктору Количество обращений в службу поддержки</em></p>
<p>Затем каждый из полученных узлов разбивается по предиктору <em>Длительность звонков в минутах</em>, и дерево останавливается в росте.</p>
<p><img src="figures/1.5.PNG" /></p>
<p><em>Рис. 1.5. Процесс последовательного разбиения набора данных</em></p>
<p>Таким образом, можно сделать вывод: если клиент обращался в службу поддержки 6 раз и более и при этом делал звонки длительностью 15 минут и более, его можно отнести к классу ушедших клиентов. Вполне возможно, что постоянно возникающие проблемы в оказании услуг (об этом свидетельствует факт частого обращения в службу поддержки) при высокой интенсивности использования сотового телефона стали причиной оттока. Ниже приводится рис. 1.6, на котором показаны все границы принятия решений, предложенные деревом для нашего набора данных.</p>
<p><img src="figures/1.6.PNG" /></p>
<p><em>Рис. 1.6. Границы принятия решений, предложенные деревом</em></p>
<p>Обратите внимание, что деревья решений применяют различные критерии для отбора предикторов и разбиения узлов в зависимости от шкалы зависимой и независимых переменных<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>. Поэтому важно задать правильные шкалы для всех переменных. В R переменные могут иметь три типа шкалы: количественную, порядковую и номинальную. Подготовку данных в Python мы будем выполнять с помощью библиотеки <code>pandas</code>, в которой переменные могут иметь два типа шкалы: количественную и категориальную.</p>
<p>Количественная шкала допускает возможность сравнения значений: мы можем узнать, на сколько и во сколько раз одно значение больше/меньше другого, при этом второй тип сравнения не всегда возможен. Среди количественных шкал выделяют шкалу интервалов, шкалу отношений и абсолютную шкалу. Шкала интервалов состоит из одинаковых интервалов и имеет условную нулевую точку (точку отсчета). Она позволяет сказать, насколько одно значение больше другого, но не позволяет сказать, во сколько раз оно больше. Например, повысив температуру с 1°C до 20°C, мы можем сказать, что температура 20°C на 19 градусов Цельсия больше 1°C, но не можем сказать, что температура 20°C в 20 раз больше, чем 1°C. Из школьного курса физики вспомним, что температура среды (например, воздуха) определяется энергией молекул, составляющих эту среду. Для идеального газа внутренняя энергия равна сумме кинетических энергий его молекул, которая, в свою очередь, пропорциональна абсолютной температуре в кельвинах. Очевидно, что, например, при «двадцатикратном» нагреве с 1°C до 20°C абсолютная температура изменится всего в (273 + 20) / (273 + 1) = 1,069 раза. Ноль по шкале Цельсия условен и соответствует 273К. Шкала отношений отличается от шкалы интервалов тем, что имеет естественную нулевую точку. Она позволяет сказать, насколько одно значение больше другого и во сколько раз оно больше. Примером шкалы отношений может служить переменная <em>Возраст</em>: мы знаем, что расстояние между 25 и 30 в два раза меньше, чем расстояние между 30 и 40, 30-летний на 5 лет старше 25-летнего. Шкалы большинства физических величин (длина, масса, сила, давление, скорость и др.) являются шкалами отношений. При этом единица измерения в этих шкалах может быть произвольной. Например, возраст можно измерять в годах, месяцах, неделях. Длину мы можем измерять в километрах, милях, лье. Абсолютная шкала помимо естественной нулевой точки имеет еще и естественную общепринятую единицу измерения. Пример абсолютной шкалы – абсолютная шкала температуры или шкала Кельвина. Нуль этой шкалы отвечает полному прекращению движения молекул, т.е. самой низкой температуре, а единицей измерения является кельвин, который равен 1/273,16 части термодинамической температуры тройной точки воды. Как и шкала отношений, абсолютная шкала также позволяет сказать, насколько одно значение больше другого и во сколько раз оно больше.</p>
<p>При работе с количественными шкалами мы можем упорядочить значения по нарастанию или убыванию интенсивности определенного признака (например, по увеличению возраста): после 25 следует 30 и 30-летний старше 25-летнего. Наконец, мы можем сказать, сколько в выборке человек с тем или иным значением возраста.</p>
<p>В R количественным переменным соответствуют векторы типа <code>double</code>, когда значения представлены в виде чисел с плавающей точкой, например, доход клиента составляет 36500.60 рублей (такие векторы еще называют числовыми), и векторы типа <code>integer</code>, когда значения представлены в виде целых чисел, например, доход клиента составляет 36000 рублей (такие векторы еще называют целочисленными). Числовые и целочисленные векторы относятся к классу <code>numeric</code>. В питоновской библиотеке <code>pandas</code> количественным переменным будут соответствовать переменные типа <code>float</code> (аналог векторов типа <code>double</code> в R) и переменные типа <code>int</code> (аналог векторов типа <code>integer</code> в R).</p>
<p>Для порядковой шкалы задан лишь порядок или ранжирование значений. Допускаются сравнения между значениями, но нельзя сказать, на сколько или во сколько раз одно значение больше другого. Пример предиктора с порядковой шкалой – переменная <em>Доход</em>, принимающая значения <em>низкий</em>, <em>средний</em>, <em>высокий</em>. Здесь уже нельзя утверждать, что расстояние между значениями <em>низкий</em> и <em>средний</em> больше или меньше в определенное количество раз расстояния между значениями <em>средний</em> и <em>высокий</em>. Мы не можем утверждать, что человек со средним доходом на n-ное количество единиц богаче, чем человек с низким доходом. Однако можно упорядочить значения по нарастанию или убыванию интенсивности определенного признака: сначала следует значение <em>низкий</em>, затем значение <em>средний</em> и потом значение <em>высокий</em>. Респонденты, относящиеся к значению <em>средний</em>, обладают меньшим доходом по сравнению с респондентами, относящимися к значению <em>высокий</em>, то есть демонстрируют меньшую интенсивность признака. Также мы можем сказать, сколько в выборке человек с тем или иным уровнем дохода.</p>
<p>В R порядковым переменным соответствуют векторы, относящиеся одновременно к типам <code>ordered</code> и <code>factor</code> (наборы упорядоченных числовых кодов, предназначенных для хранения строковых значений). Их называют упорядоченными факторами. В питоновской библиотеке <code>pandas</code> для порядковой переменной не предусмотрено отдельного типа, поэтому такие переменные представляют как переменные типа <code>int</code> или переменные типа <code>object</code>.</p>
<p>Номинальная шкала содержит только информацию о количестве объектов в значениях. Пример предиктора с номинальной шкалой – переменная <em>Регион</em>, который имеет уровни <em>Алтайский край</em>, <em>Новосибирская область</em>, <em>Красноярский край</em>, <em>Кемеровская область</em>. Мы ничего не можем сказать о расстояниях между значениями или о порядке значений. Мы можем лишь судить о количестве респондентов, проживающих в каждом регионе.</p>
<p>В R номинальным переменным соответствуют векторы типа <code>factor</code> (наборы неупорядоченных числовых кодов, предназначенных для хранения строковых значений). В библиотеке <code>pandas</code> номинальным переменным будут соответствовать переменные типа <code>object</code>; также для этого можно использовать тип <code>str</code>.</p>
<p><img src="figures/1.7.PNG" /></p>
<p><em>Рис. 1.7. Три типа шкал</em></p>
</div>
<div id="ch1.2" class="section level2" number="1.2">
<h2><span class="header-section-number">1.2</span> Краткий обзор методов деревьев решений CHAID и CART</h2>
<p>В настоящее время наиболее распространенными методами деревьев решений являются CHAID и CART.</p>
<p><strong>CHAID</strong> (<em>Chi-square Automatic Interaction Detector</em> – <em>автоматический обнаружитель взаимодействий</em>) был разработан Гордоном Каасом в 1980 году и представляет собой метод на основе дерева решений, который исследует взаимосвязь между предикторами и зависимой переменной с помощью статистических тестов.</p>
<p>Каждый раз для разбиения узла выбирается предиктор, сильнее всего взаимодействующий с зависимой переменной. Перед этим категории каждого предиктора объединяются, если они не имеют между собой статистически значимых отличий по отношению к зависимой переменной. Категории, которые дают значимые отличия по зависимой переменной, рассматриваются как отдельные. Зависимая переменная может быть измерена в категориальной шкале. Предикторы могут быть только категориальными переменными (количественные переменные должны быть предварительно преобразованы в категориальные с помощью биннинга).</p>
<p>CHAID позволяет осуществлять многомерные расщепления узлов. Каждый узел при разбиении может иметь более 2 потомков, поэтому CHAID имеет тенденцию выращивать более раскидистые деревья, чем бинарные методы. Вместе с тем из-за жестких статистических критериев расщепления нередко дерево CHAID получается нереалистично коротким и тривиальным («грубое» дерево), поэтому требуется тонкая настройка уровней значимости для объединения категорий и разбиения узлов. По сравнению с другими методами CHAID характеризуется умеренным временем вычислений.</p>
<p>Помимо прочего, метод CHAID обладает собственным способом обработки пропущенных значений. Пропуски рассматриваются как отдельная фактическая категория. В ряде случаев это имеет смысл. Например, отказ отвечать на вопрос о доходе или занятости может оказаться предсказательной категорией для зависимой переменной.</p>
<p><strong>CART</strong> (<em>Classification and Regression Tree</em> – <em>деревья классификации и регрессии</em>) был разработан в 1974-1984 годах профессорами статистики Лео Брейманом (Калифорнийский университет в Беркли), Джеромом Фридманом (Стэнфордский университет), Ричардом Олшеном (Калифорнийский университет в Беркли) и Чарльзом Стоуном (Стэнфордский университет).</p>
<p>Для построения дерева метод CART использует меры неоднородности. Эти меры основаны на принципе уменьшения неоднородности в узле.</p>
<p>Каждый раз расщепление узла происходит так, чтобы узлы-потомки стали более однородными, чем его узел-родитель. В абсолютно однородном узле все наблюдения имеют одно и то же значение целевой переменной (все объекты принадлежат к одной и той же категории целевой переменной). Такой узел называют «чистым», и в нем мы получаем максимально однозначный прогноз.</p>
<p>Зависимая переменная и предикторы может быть категориальными или количественными.</p>
<p>CART позволяет выполнять только одномерные расщепления узлов. Каждый узел при разбиении может иметь только 2 потомков. Поэтому CART имеет тенденцию выращивать высокие деревья с большим количеством уровней.</p>
<p>Часто деревья CART получаются слишком детализированные, имеют много узлов и ветвей, сложны для интерпретации, при этом усложнение дерева не приводит к повышению прогностической способности дерева. Для упрощения структуры дерева и повышения качества модели в методе CART предусмотрена возможность отсечения ветвей (прунинг). Прунинг позволяет получить дерево «подходящего размера», избежать построения ветвистых, усложненных деревьев и при этом достичь лучшего качества модели.</p>
<p>Для обработки наблюдений, у которых пропущено значение в предикторе, используются суррогаты – другие предикторы, имеющие сильную корреляцию с исходной независимой переменной. Таким образом, разбиение, задаваемое суррогатом, будет наиболее близко к разбиению, задаваемому исходным предиктором, по которому имеются пропуски. Метод CART требует большее время вычислений по сравнению с другими методами.</p>
<p>Ниже на рис. 1.8 приводится таблица сходств и различий между методами CHAID и CART.</p>
<p><img src="figures/1.8.PNG" /></p>
<p><em>Рис. 1.8. Отличие методов CHAID и CART</em></p>
</div>
<div id="ch1.3" class="section level2" number="1.3">
<h2><span class="header-section-number">1.3</span> Преимущества и недостатки деревьев решений</h2>
<p>Метод деревьев решений обладает рядом преимуществ. Главное из них – это наглядность представления результатов (в виде иерархической структуры дерева). Деревья решений позволяют работать с большим числом независимых переменных. На вход можно подавать все существующие переменные, алгоритм сам выберет наиболее значимые среди них, и только они будут использованы для построения дерева (автоматический отбор предикторов). Однако при этом для некоторых методов (например, для CHAID) может потребоваться категоризация количественных предикторов, при которой теряется часть содержащейся в ней информации.</p>
<p>Деревья решений бывают более эффективны по сравнению с линейными моделями в тех случаях, когда взаимосвязи между предикторами и зависимой переменной является нелинейными, наблюдается большое количество коррелирующих между собой переменных, присутствуют взаимодействия высоких порядков. Это обусловлено тем, что деревья пытаются описать связь между переменными путем многократных разбиений по предикторам. CHAID делает это за счет расщепления сразу на несколько категорий, CART пытается уловить эту связь посредством серии бинарных делений, и это может быть менее эффективно по сравнению с подбором параметров в линейных моделях.</p>
<p>Деревья решений устойчивы к выбросам, поскольку разбиения основаны на количестве наблюдений внутри диапазонов значений, выбранных для расщепления, а не на абсолютных значениях. Например, если у нас есть наблюдение со значением 99999, дерево может создать два узла с правилами «&lt;5» и «&gt;5» и отнести наблюдение со значением 99999 в правый узел.</p>
<p>Деревья решений относительно нечувствительны к наличию мультиколлинеарности (сильной корреляции между предикторами), в то время как линейным моделям в подобной ситуации понадобится правильно подобранная регуляризация.</p>
<p>Перед построением модели не обязательно импутировать пропущенные значения, поскольку, как уже говорилось выше, деревья используют собственные процедуры обработки пропусков. Требования, выдвигаемые методом деревьев решений к распределению переменных, не являются строгими.</p>
<p>Теперь о недостатках<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>. Для деревьев решений нельзя получить простое общее прогнозное уравнение, выражающего модель (в отличие от линейных моделей).</p>
<p>Еще одним недостатком можно считать принципиальную неспособность деревьев выполнять экстраполяцию при решении задачи регрессии: предсказание для любой комбинации предикторов всегда будет средним значением в одном из листьев, т.е. за пределы диапазона значений целевой переменной в обучающей выборки выйти невозможно. Линейная модель, наоборот, может дать осмысленный прогноз (хотя качество этого прогноза не гарантируется – модели машинного обучения ведут предсказуемо себя в той области значений предикторов и целевой переменной, которые модель «видела» на этапе обучения).</p>
<p>Деревьям решений свойственно переобучение. Речь идет о ситуации, когда дерево в силу гибкости используемого алгоритма получается слишком детализированным, имеют много узлов и ветвей, сложно для интерпретации, что требует специальной процедуры отсечения. Небольшие изменения в наборе данных могут приводить к построению совершенно другого дерева, потому что в силу иерархичности дерева изменения в верхних узлах ведут к изменениям во всех узлах, расположенных ниже.</p>
<p>Отметим, что в большей степени проблемы переобучения и нестабильности относится к методу CART (в CHAID эти проблемы во многом снимаются за счет использования строгих статистических критериев). Предпринимая попытки улучшить прогностическую способность и стабильность метода CART, один из его разработчиков Лео Брейман пришел к идее случайного леса, когда из обучающего набора извлекаются случайные выборки (того же объема, что и исходный обучающий набор) с возвращением, по каждой строится дерево с использованием случайно отобранных предикторов и затем результаты, полученные по каждому дереву, усредняются. Однако при таком подходе теряется главное преимущество деревьев решений – простота интерпретации.</p>
<p>Наконец, для методов одиночных деревьев характерна проблема множественных сравнений. Перед расщеплением узла дерево сравнивает различные варианты разбиения, число этих вариантов зависит от числа уровней предикторов, поэтому, как правило, происходит смещение выбора в пользу переменных, у которых большее количество уровней.</p>
<p><img src="figures/1.9.PNG" /></p>
<p><em>Рис. 1.9. Преимущества и недостатки деревьев решений </em></p>
</div>
<div id="ch1.4" class="section level2" number="1.4">
<h2><span class="header-section-number">1.4</span> Задачи, выполняемые с помощью деревьев решений</h2>
<p>Деревья применяются для задач регрессии и классификации в различных областях – от банковской сферы до медицины. Также они могут быть использованы, например, сегментации клиентской базы: определив, какие демографические группы имеют максимальный показатель отклика, мы можем использовать эту информацию, чтобы максимизировать отклик при будущей прямой рассылке.</p>
<p>Здесь необходимо отметить, что в настоящее время дерево решений CART все реже используется для построения самостоятельной прогнозной модели в силу вышеописанных недостатков, а построение модели на основе дерева CHAID должно сопровождаться тщательно подготовленными процедурами проверки. Часто дерево решений CHAID используют в ансамбле с логистической регрессией. Например, ансамбли дерева решений CHAID и логистической регрессии используются в моделях оттока в телекоме, что позволяет поднять качество модели, при этом, в отличие от случайного леса и градиентного бустинга, такой ансамбль легко интерпретируем.</p>
<p>В банковском скоринге деревья решений используются как вспомогательный инструмент при разработке модели логистической регрессии. Приведем конкретные примеры такого применения дерева.</p>
<p>В кредитном скоринге использование нескольких скоринговых карт для одного портфеля обеспечивает лучшее дифференцирование риска, чем использование одной скоринговой карты. Это характерно, когда нам приходится работать с разнородной аудиторией, состоящей из различных групп, и одна и та же скоринговая карта не может работать достаточно эффективно для всех. Например, в скоринге кредитных карточек выделяют сегменты «активные клиенты» и «неактивные клиенты», «клиенты в просрочке» и «клиенты, не имеющие просрочек». Переменные в таких сегментах будут сильно различаться. Например, для активных кредитных карт утилизация будет сильной переменной, а для неактивных – слабой. И, наоборот, может оказаться, что время неактивности для активных клиентов равно 0, а для неактивных клиентов время неактивности окажется сильной переменной. Для этих целей выполняют сегментацию клиентов. Первый способ сегментации – деление на группы на основе опыта и отраслевых знаний с последующей аналитической проверкой. Второй способ – это сегментация с помощью статистических методов типа кластерного анализа или деревьев решений. При этом по сравнению с кластерным анализом деревья решений обладают преимуществом: они формулируют четкие правила выделения сегментов. В дальнейшем для каждого из сегментов можно построить собственную модель логистической регрессии, разработать скоринговую карту и сформулировать кредитные правила. В Citibank USA является стандартной практикой делать дерево с двумя-тремя уровнями и в каждом узле подгонять свою модель логистической регрессии. В основе скорингового балла FICO также лежит сегментация на основе деревьев решений. Об эффективности использования сегментации в кредитном скоринге пишет в своей книге «Скоринговые карты для оценки кредитных рисков» известный эксперт по управления рисками Наим Сиддики<a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>, а также один из разработчиков алгоритмов скоринга компании FICO Брюс Ходли<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a>.</p>
<p>С помощью деревьев решений из большого числа предикторов можно выбрать переменные, полезные для построения модели логистической регрессии). Например, из 100 переменных дерево включило в модель 25 переменных, таким образом, у нас появляется информация о том, какие переменные наверняка можно включить в модель логистической регрессии. Методы CART и случайный лес позволяют вычислить важность переменных, использованных в модели, благодаря чему мы можем ранжировать переменные по степени полезности.</p>
<p>Деревья решений можно использовать для биннинга – перегруппировки категориального предиктора или дискретизации количественного предиктора с целью лучшего описания взаимосвязи с зависимой переменной. Например, при построении модели логистической регрессии часто обнаруживается, что взаимосвязи между количественным предиктором и интересующим событием являются нелинейными. Уравнение логистической регрессии, несмотря на нелинейное преобразование своего выходного значения (логит-преобразование), все равно моделирует линейные зависимости между предикторами и зависимой переменной. Возьмем пример нелинейной зависимости между стажем работы в банке и внутренним мошенничеством. Допустим, рассчитанный регрессионный коэффициент в уравнении логистической регрессии получился отрицательным. Это значит, что вероятность совершения внутреннего мошенничества с увеличением стажа работы уменьшается. Однако, после выполнения разбивки переменной с помощью дерева CHAID на категории до 12 месяцев, от 12 до 36 месяцев, от 36 до 60 месяцев и больше 60 месяцев, стало видно, что зависимость между стажем и внутренним мошенничеством нелинейная. Первая (до 12 месяцев) и последняя (больше 60 месяцев) категории склонны к внутреннему мошенничеству, а промежуточные сегменты, наоборот, не склонны к внутреннему мошенничеству. После правильной разбивки переменной, проведенной с помощью дерева, связь между предиктором и зависимой переменной становится больше похожа на реальную.</p>
<p>Строя модель логистической регрессии, нередко приходится работать с предикторами, у которых большое количество категорий. Как правило, речь идет о географических переменных (регион, область регистрации, область фактического пребывания заемщика, область торговой точки, где клиент брал кредит) и переменных, фиксирующих профессию или сферу занятости заемщика. При этом некоторые категории могут быть редкими и либо станут источником шума, либо вообще вызовут проблемы моделирования (когда наблюдения, относящиеся к редкой категории, не попадут в обучающую выборку, но встретятся в контрольной). Исключение таких переменных из анализа также нерационально, поскольку они могут дать ценную информацию. Поэтому можно выполнить биннинг с целью укрупнения категорий, а можно построить по такой переменной дерево решений. В результате дерево укрупнит категории переменных и скомбинирует переменные так, чтобы полученные комбинации характеристик максимизировали различия по зависимой переменной. Такую переменную, где категориями являются терминальные узлы дерева, можно включить в модель логистической регрессии.</p>
</div>
</div>

<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>Поэтому зависимую переменную еще называют объясняемой переменной, а предикторы – объясняющими переменными. Кроме того, в качестве синонима «зависимая переменная» могут использоваться термины «целевая переменная», «результирующая переменная», «отклик», «выходная переменная». Предикторы также называют факторами, входными переменными.<a href="ch1.html#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>Подробнее о шкалах см., например, по ссылке <a href="http://www.aiportal.ru/articles/autoclassification/type-scales.html" class="uri">http://www.aiportal.ru/articles/autoclassification/type-scales.html</a>.<a href="ch1.html#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>Более подробное обсуждение этой темы можно найти по ссылке <a href="https://stats.stackexchange.com/questions/1292/what-is-the-weak-side-of-decision-trees" class="uri">https://stats.stackexchange.com/questions/1292/what-is-the-weak-side-of-decision-trees</a><a href="ch1.html#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>Наим Сиддики. Скоринговые карты для оценки кредитных рисков. М.: Манн, Иванов и Фабер, 2014.<a href="ch1.html#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>Breiman, L. 2001. Arcing classifiers (with discussion). The Annals of Statistics, vol.26, no. 3, 801–849. 2001b. Statistical modeling: The two cultures (with rejoinders). Statistical Science, vol. 16, no. 3, 199–231.<a href="ch1.html#fnref5" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="ch2.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
